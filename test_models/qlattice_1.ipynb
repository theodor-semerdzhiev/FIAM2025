{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc952e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FEYN QLattice — Rolling yearly training (2017–2025)\n",
    "# Train on 2007..(T-3), validate on {T-2, T-1}, test on T\n",
    "# Logs everything, saves ALL candidates + formulas, marks BEST\n",
    "# Memory-lean: strategic gc.collect(), float32 features, minimized temporaries\n",
    "# ============================================\n",
    "\n",
    "import sys, subprocess, warnings, time, os, pickle, json, shutil, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "# -------- Paths (top-level) --------\n",
    "LOG_PATH                 = \"feyn_qlattice_train.log\"\n",
    "BASE_MODELS_DIR          = \"qlattice_models\"                 # per-year subfolders of raw candidates\n",
    "BASE_OUTPUT_DIR          = \"qlattice_yearly\"                 # per-year artifacts (best model, metrics, preds)\n",
    "AGG_METRICS_CSV          = \"qlattice_yearly_summary.csv\"     # one row per test year\n",
    "\n",
    "# -------- Data / columns --------\n",
    "PATH               = \"../data/ret_sample.parquet\"\n",
    "ID_COL             = \"id\"\n",
    "DATE_COL           = \"date\"\n",
    "TARGET             = \"stock_ret\"\n",
    "RANDOM_SEED        = 42\n",
    "TRAIN_START_YEAR   = 2007\n",
    "TEST_YEARS         = list(range(2017, 2026))  # 2017..2025 inclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea56c10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:33:00] Checking feyn/QLattice...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "This version of Feyn and the QLattice is available for academic, personal, and non-commercial use. By using the community version of this software you agree to the terms and conditions which can be found at https://abzu.ai/eula."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:33:00] QLattice import OK.\n",
      "[23:33:00] Reading parquet: ../data/ret_sample.parquet\n",
      "[23:33:20] Read done. Rows=6,401,414, Cols=159\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============== Logging ==============\n",
    "def log(msg: Optional[str] = None) -> None:\n",
    "    if not hasattr(log, \"t0\"):\n",
    "        log.t0 = time.time()\n",
    "        with open(LOG_PATH, \"w\") as f:\n",
    "            f.write(f\"[{time.strftime('%H:%M:%S')}] Log started\\n\")\n",
    "    if msg:\n",
    "        ts = f\"[{time.strftime('%H:%M:%S')}] {msg}\"\n",
    "        print(ts, flush=True)\n",
    "        with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(ts + \"\\n\")\n",
    "\n",
    "def save_text(path: str, text: str) -> None:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    log(f\"Wrote text file: {path}  (size={len(text)} chars)\")\n",
    "\n",
    "def save_pickle(path: str, obj: Any) -> None:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    log(f\"Wrote pickle: {path}\")\n",
    "\n",
    "def save_json(path: str, obj: Any) -> None:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "    log(f\"Wrote JSON: {path}\")\n",
    "\n",
    "def save_csv_df(path: str, df: pd.DataFrame) -> None:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    df.to_csv(path, index=False)\n",
    "    log(f\"Wrote CSV: {path}  (rows={len(df)})\")\n",
    "\n",
    "def _try_symlink(src: str, dst: str) -> None:\n",
    "    try:\n",
    "        if os.path.islink(dst) or os.path.exists(dst):\n",
    "            try: os.remove(dst)\n",
    "            except Exception: pass\n",
    "        os.symlink(src, dst)\n",
    "        log(f\"Created symlink: {dst} -> {src}\")\n",
    "    except Exception:\n",
    "        shutil.copyfile(src, dst)\n",
    "        log(f\"Symlink not supported; copied instead: {dst}\")\n",
    "\n",
    "def _mark_best_candidate(year_models_dir: str, best_idx: int) -> None:\n",
    "    marker_txt = os.path.join(year_models_dir, f\"BEST_INDEX_{best_idx:03d}.txt\")\n",
    "    with open(marker_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(str(best_idx) + \"\\n\")\n",
    "    log(f\"Wrote best-index marker: {marker_txt}\")\n",
    "\n",
    "# ============== Feyn import ==============\n",
    "log(\"Checking feyn/QLattice...\")\n",
    "try:\n",
    "    from feyn import QLattice\n",
    "    log(\"QLattice import OK.\")\n",
    "except Exception:\n",
    "    log(\"Installing feyn (quiet)...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"feyn\"])\n",
    "    from feyn import QLattice\n",
    "    log(\"QLattice installed & imported.\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ============== Load ==============\n",
    "log(f\"Reading parquet: {PATH}\")\n",
    "df = pd.read_parquet(PATH)\n",
    "log(f\"Read done. Rows={len(df):,}, Cols={len(df.columns)}\")\n",
    "gc.collect()\n",
    "\n",
    "# Ensure datetime, year\n",
    "if DATE_COL in df and not np.issubdtype(df[DATE_COL].dtype, np.datetime64):\n",
    "    log(f\"Parsing {DATE_COL} to datetime...\")\n",
    "    df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "\n",
    "if \"year\" not in df.columns:\n",
    "    if DATE_COL in df:\n",
    "        log(\"Deriving 'year' from date column...\")\n",
    "        df[\"year\"] = df[DATE_COL].dt.year\n",
    "    else:\n",
    "        raise RuntimeError(\"No 'year' or parsable DATE_COL found to create splits.\")\n",
    "gc.collect()\n",
    "\n",
    "# ============== Helpers ==============\n",
    "# Drop-likes never used as features\n",
    "drop_like = {ID_COL, DATE_COL, TARGET, \"year\", \"month\", \"char_date\", \"gvkey\", \"iid\", \"ret_eom\", \"char_eom\"}\n",
    "\n",
    "def _make_ql() -> QLattice:\n",
    "    try:\n",
    "        return QLattice(random_seed=RANDOM_SEED)\n",
    "    except TypeError:\n",
    "        try: return QLattice(random_state=RANDOM_SEED)\n",
    "        except TypeError: return QLattice()\n",
    "\n",
    "def fit_candidates(df_small: pd.DataFrame, yname: str):\n",
    "    ql = _make_ql()\n",
    "    log(\"QLattice.auto_run(...)\")\n",
    "    models = ql.auto_run(df_small, yname, max_complexity=12, n_epochs=15, criterion=\"bic\")\n",
    "    log(f\"Got {len(models)} candidate models.\")\n",
    "    return models\n",
    "\n",
    "def predict_safely(model: Any, df_small: pd.DataFrame, yname: str, xcols: List[str]) -> np.ndarray:\n",
    "    # Avoid building new DataFrames repeatedly; rely on column-order selection when possible\n",
    "    try:\n",
    "        arr = model.predict(df_small[[yname] + xcols])\n",
    "        preds = np.asarray(arr, dtype=np.float32, order=\"C\")\n",
    "        log(f\"Prediction used `[y]+X` signature (rows={preds.shape[0]}).\")\n",
    "        return preds\n",
    "    except Exception:\n",
    "        arr = model.predict(df_small[xcols])\n",
    "        preds = np.asarray(arr, dtype=np.float32, order=\"C\")\n",
    "        log(f\"Prediction used `X-only` signature (rows={preds.shape[0]}).\")\n",
    "        return preds\n",
    "\n",
    "def summary_stats(y: np.ndarray, p: np.ndarray, prefix: str) -> Dict[str, Any]:\n",
    "    # y, p are float arrays; keep as float32 to reduce memory; cast for stats where needed\n",
    "    y32 = y.astype(np.float32, copy=False)\n",
    "    p32 = p.astype(np.float32, copy=False)\n",
    "    resid = p32 - y32\n",
    "    # compute with float64 for numeric stability on aggregates, then cast to float for JSON\n",
    "    mse = float(np.nanmean(resid.astype(np.float64) ** 2))\n",
    "    mae = float(np.nanmean(np.abs(resid.astype(np.float64))))\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    out: Dict[str, Any] = {\n",
    "        f\"{prefix}_n\": int(np.isfinite(y32).sum()),\n",
    "        f\"{prefix}_mse\": mse,\n",
    "        f\"{prefix}_mae\": mae,\n",
    "        f\"{prefix}_rmse\": rmse,\n",
    "    }\n",
    "    try:\n",
    "        r, _ = pearsonr(y32.astype(np.float64), p32.astype(np.float64))\n",
    "        out[f\"{prefix}_pearson_r\"] = float(r)\n",
    "    except Exception as e:\n",
    "        log(f\"[warn] pearsonr failed: {e}\")\n",
    "        out[f\"{prefix}_pearson_r\"] = float(\"nan\")\n",
    "    try:\n",
    "        ic = spearmanr(y32, p32, nan_policy=\"omit\").correlation\n",
    "        out[f\"{prefix}_spearman_ic\"] = float(ic) if ic is not None else float(\"nan\")\n",
    "    except Exception as e:\n",
    "        log(f\"[warn] spearmanr failed: {e}\")\n",
    "        out[f\"{prefix}_spearman_ic\"] = float(\"nan\")\n",
    "    ybar = float(np.nanmean(y32.astype(np.float64)))\n",
    "    ss_tot = float(np.nansum((y32.astype(np.float64) - ybar) ** 2))\n",
    "    ss_res = float(np.nansum((y32.astype(np.float64) - p32.astype(np.float64)) ** 2))\n",
    "    out[f\"{prefix}_r2\"] = float(\"nan\") if ss_tot == 0.0 else 1.0 - (ss_res / ss_tot)\n",
    "    return out\n",
    "\n",
    "def safe_formula(model: Any) -> str:\n",
    "    try:\n",
    "        s = str(model.sympify())\n",
    "        if not isinstance(s, str):\n",
    "            s = str(s)\n",
    "        log(f\"Extracted symbolic representation (len={len(s)}).\")\n",
    "        return s\n",
    "    except Exception as e:\n",
    "        log(f\"[warn] Could not extract symbolic formula: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def _downcast_float32_inplace(df_small: pd.DataFrame, feat_cols: List[str]) -> None:\n",
    "    # Downcast features to float32 (saves ~2x memory); TARGET stays original dtype\n",
    "    for c in feat_cols:\n",
    "        if pd.api.types.is_float_dtype(df_small[c]):\n",
    "            df_small[c] = df_small[c].astype(np.float32, copy=False)\n",
    "        elif pd.api.types.is_integer_dtype(df_small[c]):\n",
    "            df_small[c] = df_small[c].astype(np.float32, copy=False)\n",
    "\n",
    "def standardize_fit_transform(train_df_small: pd.DataFrame, val_df_small: pd.DataFrame, test_df_small: pd.DataFrame, feat_cols: List[str]):\n",
    "    # compute mu/sd in float64 (stability), then apply in-place on float32 arrays\n",
    "    mu = np.empty(len(feat_cols), dtype=np.float64)\n",
    "    sd = np.empty(len(feat_cols), dtype=np.float64)\n",
    "\n",
    "    for j, c in enumerate(feat_cols):\n",
    "        arr = train_df_small[c].to_numpy(dtype=np.float64, copy=False)\n",
    "        mu[j] = np.nanmean(arr)\n",
    "        sd[j] = np.nanstd(arr, ddof=1)\n",
    "\n",
    "    np.nan_to_num(sd, copy=False, nan=1.0)\n",
    "    sd[~np.isfinite(sd)] = 1.0\n",
    "    sd[sd == 0.0] = 1.0\n",
    "\n",
    "    # In-place z-score; we avoid allocating new DataFrames\n",
    "    for j, c in enumerate(feat_cols):\n",
    "        # train\n",
    "        t = train_df_small[c].to_numpy(dtype=np.float32, copy=False)\n",
    "        np.subtract(t, mu[j], out=t, dtype=np.float32)\n",
    "        np.divide(t, sd[j], out=t, dtype=np.float32)\n",
    "        # val\n",
    "        v = val_df_small[c].to_numpy(dtype=np.float32, copy=False)\n",
    "        np.subtract(v, mu[j], out=v, dtype=np.float32)\n",
    "        np.divide(v, sd[j], out=v, dtype=np.float32)\n",
    "        # test\n",
    "        w = test_df_small[c].to_numpy(dtype=np.float32, copy=False)\n",
    "        np.subtract(w, mu[j], out=w, dtype=np.float32)\n",
    "        np.divide(w, sd[j], out=w, dtype=np.float32)\n",
    "\n",
    "    # Fill residual NaNs with 0 in-place\n",
    "    train_df_small[feat_cols] = train_df_small[feat_cols].fillna(0.0)\n",
    "    val_df_small[feat_cols]   = val_df_small[feat_cols].fillna(0.0)\n",
    "    test_df_small[feat_cols]  = test_df_small[feat_cols].fillna(0.0)\n",
    "\n",
    "    log(f\"Standardized & imputed: features={len(feat_cols)} | \"\n",
    "        f\"train={train_df_small.shape} val={val_df_small.shape} test={test_df_small.shape}\")\n",
    "    return mu, sd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d2a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:37:18] Got 10 candidate models.\n",
      "[04:37:18] Evaluating 10 candidates for year 2025 (selection by VALIDATION MSE)...\n",
      "[04:37:19] Prediction used `[y]+X` signature (rows=4975091).\n",
      "[04:37:20] Prediction used `[y]+X` signature (rows=727615).\n",
      "[04:37:20] Prediction used `[y]+X` signature (rows=164546).\n",
      "[04:37:21] Wrote pickle: qlattice_models/2025/model_000.pkl\n",
      "[04:37:21] Extracted symbolic representation (len=45).\n",
      "[04:37:21] Wrote text file: qlattice_models/2025/model_000_formula.txt  (size=46 chars)\n",
      "[04:37:21] [2025] Model 0: Val MSE = 500.58\n",
      "[04:37:22] Prediction used `[y]+X` signature (rows=4975091).\n",
      "[04:37:22] Prediction used `[y]+X` signature (rows=727615).\n",
      "[04:37:22] Prediction used `[y]+X` signature (rows=164546).\n",
      "[04:37:23] Wrote pickle: qlattice_models/2025/model_001.pkl\n",
      "[04:37:23] Extracted symbolic representation (len=26).\n",
      "[04:37:23] Wrote text file: qlattice_models/2025/model_001_formula.txt  (size=27 chars)\n",
      "[04:37:23] [2025] Model 1: Val MSE = 506.892\n",
      "[04:37:24] Prediction used `[y]+X` signature (rows=4975091).\n",
      "[04:37:24] Prediction used `[y]+X` signature (rows=727615).\n",
      "[04:37:24] Prediction used `[y]+X` signature (rows=164546).\n",
      "[04:37:25] Wrote pickle: qlattice_models/2025/model_002.pkl\n",
      "[04:37:25] Extracted symbolic representation (len=34).\n",
      "[04:37:25] Wrote text file: qlattice_models/2025/model_002_formula.txt  (size=35 chars)\n",
      "[04:37:25] [2025] Model 2: Val MSE = 507.127\n",
      "[04:37:26] Prediction used `[y]+X` signature (rows=4975091).\n",
      "[04:37:26] Prediction used `[y]+X` signature (rows=727615).\n",
      "[04:37:26] Prediction used `[y]+X` signature (rows=164546).\n",
      "[04:37:27] Wrote pickle: qlattice_models/2025/model_003.pkl\n",
      "[04:37:28] Extracted symbolic representation (len=25).\n",
      "[04:37:28] Wrote text file: qlattice_models/2025/model_003_formula.txt  (size=26 chars)\n",
      "[04:37:28] [2025] Model 3: Val MSE = 506.69\n",
      "[04:37:28] Prediction used `[y]+X` signature (rows=4975091).\n",
      "[04:37:29] Prediction used `[y]+X` signature (rows=727615).\n",
      "[04:37:29] Prediction used `[y]+X` signature (rows=164546).\n",
      "[04:37:30] Wrote pickle: qlattice_models/2025/model_004.pkl\n",
      "[04:37:30] Extracted symbolic representation (len=36).\n",
      "[04:37:30] Wrote text file: qlattice_models/2025/model_004_formula.txt  (size=37 chars)\n",
      "[04:37:30] [2025] Model 4: Val MSE = 507.137\n",
      "[04:37:31] Prediction used `[y]+X` signature (rows=4975091).\n",
      "[04:37:31] Prediction used `[y]+X` signature (rows=727615).\n",
      "[04:37:31] Prediction used `[y]+X` signature (rows=164546).\n",
      "[04:37:32] Wrote pickle: qlattice_models/2025/model_005.pkl\n",
      "[04:37:32] Extracted symbolic representation (len=37).\n",
      "[04:37:32] Wrote text file: qlattice_models/2025/model_005_formula.txt  (size=38 chars)\n",
      "[04:37:32] [2025] Model 5: Val MSE = 507.136\n",
      "[04:37:33] Prediction used `[y]+X` signature (rows=4975091).\n",
      "[04:37:33] Prediction used `[y]+X` signature (rows=727615).\n",
      "[04:37:33] Prediction used `[y]+X` signature (rows=164546).\n",
      "[04:37:34] Wrote pickle: qlattice_models/2025/model_006.pkl\n",
      "[04:37:34] Extracted symbolic representation (len=28).\n",
      "[04:37:34] Wrote text file: qlattice_models/2025/model_006_formula.txt  (size=29 chars)\n",
      "[04:37:34] [2025] Model 6: Val MSE = 507.119\n",
      "[04:37:35] Prediction used `[y]+X` signature (rows=4975091).\n",
      "[04:37:35] Prediction used `[y]+X` signature (rows=727615).\n",
      "[04:37:35] Prediction used `[y]+X` signature (rows=164546).\n",
      "[04:37:37] Wrote pickle: qlattice_models/2025/model_007.pkl\n",
      "[04:37:37] Extracted symbolic representation (len=30).\n",
      "[04:37:37] Wrote text file: qlattice_models/2025/model_007_formula.txt  (size=31 chars)\n",
      "[04:37:37] [2025] Model 7: Val MSE = 507.136\n",
      "[04:37:38] Prediction used `[y]+X` signature (rows=4975091).\n",
      "[04:37:38] Prediction used `[y]+X` signature (rows=727615).\n",
      "[04:37:38] Prediction used `[y]+X` signature (rows=164546).\n",
      "[04:37:39] Wrote pickle: qlattice_models/2025/model_008.pkl\n",
      "[04:37:39] Extracted symbolic representation (len=56).\n",
      "[04:37:39] Wrote text file: qlattice_models/2025/model_008_formula.txt  (size=57 chars)\n",
      "[04:37:39] [2025] Model 8: Val MSE = 507.127\n",
      "[04:37:40] Prediction used `[y]+X` signature (rows=4975091).\n",
      "[04:37:40] Prediction used `[y]+X` signature (rows=727615).\n",
      "[04:37:40] Prediction used `[y]+X` signature (rows=164546).\n",
      "[04:37:42] Wrote pickle: qlattice_models/2025/model_009.pkl\n",
      "[04:37:42] Extracted symbolic representation (len=31).\n",
      "[04:37:42] Wrote text file: qlattice_models/2025/model_009_formula.txt  (size=32 chars)\n",
      "[04:37:42] [2025] Model 9: Val MSE = 507.139\n",
      "[04:37:42] Wrote JSON: qlattice_yearly/2025/qlattice_candidates_metrics_2025.json\n",
      "[04:37:42] Wrote CSV: qlattice_yearly/2025/qlattice_candidates_metrics_2025.csv  (rows=10)\n",
      "[04:37:42] Wrote text file: qlattice_yearly/2025/qlattice_candidates_formulas_2025.txt  (size=527 chars)\n",
      "[04:37:42] [2025] Selected best model index 0 by lowest validation MSE = 500.58\n",
      "[04:37:42] Wrote best-index marker: qlattice_models/2025/BEST_INDEX_000.txt\n",
      "[04:37:42] [2025] Created BEST copies: BEST__model_000.pkl, BEST__model_000_formula.txt\n",
      "[04:37:42] Created symlink: qlattice_models/2025/best_model.pkl -> qlattice_models/2025/model_000.pkl\n",
      "[04:37:42] Created symlink: qlattice_models/2025/best_model_formula.txt -> qlattice_models/2025/model_000_formula.txt\n",
      "[04:37:42] Wrote JSON: qlattice_models/2025/best_model_meta_2025.json\n",
      "[04:37:42] [2025] Reloaded BEST model from qlattice_models/2025/model_000.pkl\n",
      "[04:37:42] Extracted symbolic representation (len=45).\n",
      "[04:37:42] Wrote text file: qlattice_yearly/2025/qlattice_formulas_2025.txt  (size=46 chars)\n",
      "[04:37:42] Wrote pickle: qlattice_yearly/2025/qlattice_model_2025.pkl\n",
      "[04:37:42] Wrote NPZ: qlattice_yearly/2025/qlattice_preproc_2025.npz  (feat_cols=148)\n",
      "[04:37:43] Prediction used `[y]+X` signature (rows=4975091).\n",
      "[04:37:43] Prediction used `[y]+X` signature (rows=727615).\n",
      "[04:37:43] Prediction used `[y]+X` signature (rows=164546).\n",
      "[04:37:44] Wrote JSON: qlattice_yearly/2025/qlattice_metrics_2025.json\n",
      "[04:37:44] Wrote CSV: qlattice_yearly/2025/qlattice_metrics_2025.csv  (rows=1)\n",
      "[04:37:48] Wrote CSV: qlattice_yearly/2025/qlattice_train_predictions_2025.csv  (rows=4975091)\n",
      "[04:37:49] Wrote CSV: qlattice_yearly/2025/qlattice_val_predictions_2025.csv  (rows=727615)\n",
      "[04:37:49] Wrote CSV: qlattice_yearly/2025/qlattice_test_predictions_2025.csv  (rows=164546)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 270\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;66;03m# ============== Aggregate summary across all test years ==============\u001b[39;00m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m agg_rows:\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[43msave_csv_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAGG_METRICS_CSV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43magg_rows\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msort_values\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43myear\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    272\u001b[39m     log(\u001b[33m\"\u001b[39m\u001b[33mNo years processed successfully; aggregate CSV not created.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36msave_csv_df\u001b[39m\u001b[34m(path, df)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_csv_df\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, df: pd.DataFrame) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     df.to_csv(path, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     34\u001b[39m     log(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWrote CSV: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  (rows=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:228\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "# ============== Rolling loop ==============\n",
    "os.makedirs(BASE_MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)\n",
    "gc.collect()\n",
    "\n",
    "agg_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "for T in TEST_YEARS:\n",
    "    # Example: T=2018 -> val = 2016–2017, train = 2007–2015, test = 2018\n",
    "    val_lo, val_hi = T - 2, T - 1\n",
    "    train_hi = T - 3\n",
    "\n",
    "    log(\"=\"*72)\n",
    "    log(f\"=== Processing TEST YEAR {T} \"\n",
    "        f\"(Train {TRAIN_START_YEAR}..{train_hi}, Val in [{val_lo}, {val_hi}], Test=={T}) ===\")\n",
    "\n",
    "    # Define splits (no extra copies here; masks create views until we mutate)\n",
    "    train_mask = (df[\"year\"] >= TRAIN_START_YEAR) & (df[\"year\"] <= train_hi)\n",
    "    val_mask   = (df[\"year\"] >= val_lo) & (df[\"year\"] <= val_hi)\n",
    "    test_mask  = (df[\"year\"] == T)\n",
    "\n",
    "    train_df = df.loc[train_mask]\n",
    "    val_df   = df.loc[val_mask]\n",
    "    test_df  = df.loc[test_mask]\n",
    "    log(f\"Split sizes — train={len(train_df):,} val={len(val_df):,} test={len(test_df):,}\")\n",
    "\n",
    "    if train_df.empty or val_df.empty or test_df.empty:\n",
    "        log(f\"Skipping {T}: missing data in one or more splits.\")\n",
    "        del train_df, val_df, test_df\n",
    "        gc.collect()\n",
    "        continue\n",
    "\n",
    "    # Sort (if columns exist)\n",
    "    if {ID_COL, DATE_COL} <= set(train_df.columns): train_df = train_df.sort_values([ID_COL, DATE_COL])\n",
    "    if {ID_COL, DATE_COL} <= set(val_df.columns):   val_df   = val_df.sort_values([ID_COL, DATE_COL])\n",
    "    if {ID_COL, DATE_COL} <= set(test_df.columns):  test_df  = test_df.sort_values([ID_COL, DATE_COL])\n",
    "\n",
    "    # Feature selection (numeric, intersect across splits)\n",
    "    num_cols_train = [c for c in train_df.select_dtypes(include=[np.number]).columns if c not in drop_like]\n",
    "    num_cols_val   = [c for c in val_df.select_dtypes(include=[np.number]).columns   if c not in drop_like]\n",
    "    num_cols_test  = [c for c in test_df.select_dtypes(include=[np.number]).columns  if c not in drop_like]\n",
    "    feat_cols = [c for c in num_cols_train if (c in num_cols_val) and (c in num_cols_test)]\n",
    "    log(f\"Selected {len(feat_cols)} overlapping numeric features for year {T}.\")\n",
    "\n",
    "    if not feat_cols:\n",
    "        log(f\"Skipping {T}: no overlapping numeric features across splits.\")\n",
    "        del train_df, val_df, test_df\n",
    "        gc.collect()\n",
    "        continue\n",
    "\n",
    "    cols = [TARGET] + feat_cols\n",
    "\n",
    "    # Build small frames (views until we write); immediately drop NaN targets to keep them small\n",
    "    train_small = train_df.loc[:, cols]\n",
    "    val_small   = val_df.loc[:, cols]\n",
    "    test_small  = test_df.loc[:, cols]\n",
    "    del train_df, val_df, test_df\n",
    "    gc.collect()\n",
    "\n",
    "    train_small = train_small[train_small[TARGET].notna()]\n",
    "    val_small   = val_small[val_small[TARGET].notna()]\n",
    "    test_small  = test_small[test_small[TARGET].notna()]\n",
    "    log(f\"After dropping NaN targets — train={len(train_small):,} val={len(val_small):,} test={len(test_small):,}\")\n",
    "\n",
    "    if train_small.empty or val_small.empty or test_small.empty:\n",
    "        del train_small, val_small, test_small\n",
    "        gc.collect()\n",
    "        log(f\"Skipping {T}: empty split after dropping NaN targets.\")\n",
    "        continue\n",
    "\n",
    "    # Downcast features to float32 (saves memory)\n",
    "    _downcast_float32_inplace(train_small, feat_cols)\n",
    "    _downcast_float32_inplace(val_small,   feat_cols)\n",
    "    _downcast_float32_inplace(test_small,  feat_cols)\n",
    "    gc.collect()\n",
    "\n",
    "    # Standardize using TRAIN stats only; impute z-space NaNs with 0 (in-place)\n",
    "    mu, sd = standardize_fit_transform(train_small, val_small, test_small, feat_cols)\n",
    "    gc.collect()\n",
    "\n",
    "    # Fit candidates on TRAIN\n",
    "    candidates = fit_candidates(train_small, TARGET)\n",
    "    if not candidates:\n",
    "        log(f\"Skipping {T}: no candidates returned by QLattice.\")\n",
    "        del train_small, val_small, test_small\n",
    "        gc.collect()\n",
    "        continue\n",
    "\n",
    "    # Per-year output dirs / filenames\n",
    "    year_models_dir = os.path.join(BASE_MODELS_DIR, f\"{T}\")\n",
    "    os.makedirs(year_models_dir, exist_ok=True)\n",
    "    year_out_dir = os.path.join(BASE_OUTPUT_DIR, f\"{T}\")\n",
    "    os.makedirs(year_out_dir, exist_ok=True)\n",
    "\n",
    "    CANDIDATE_METRICS_JSON = os.path.join(year_out_dir, f\"qlattice_candidates_metrics_{T}.json\")\n",
    "    CANDIDATE_METRICS_CSV  = os.path.join(year_out_dir, f\"qlattice_candidates_metrics_{T}.csv\")\n",
    "    ALL_FORMULAS_TXT       = os.path.join(year_out_dir, f\"qlattice_candidates_formulas_{T}.txt\")\n",
    "    MODEL_OUT    = os.path.join(year_out_dir, f\"qlattice_model_{T}.pkl\")\n",
    "    PREPROC_OUT  = os.path.join(year_out_dir, f\"qlattice_preproc_{T}.npz\")\n",
    "    FORMULAS_OUT = os.path.join(year_out_dir, f\"qlattice_formulas_{T}.txt\")\n",
    "    METRICS_JSON = os.path.join(year_out_dir, f\"qlattice_metrics_{T}.json\")\n",
    "    METRICS_CSV  = os.path.join(year_out_dir, f\"qlattice_metrics_{T}.csv\")\n",
    "    TRAIN_PRED_CSV = os.path.join(year_out_dir, f\"qlattice_train_predictions_{T}.csv\")\n",
    "    VAL_PRED_CSV   = os.path.join(year_out_dir, f\"qlattice_val_predictions_{T}.csv\")\n",
    "    TEST_PRED_CSV  = os.path.join(year_out_dir, f\"qlattice_test_predictions_{T}.csv\")\n",
    "\n",
    "    # Evaluate all candidates on VAL; pick best by Val MSE; save each model + formula\n",
    "    y_train = train_small[TARGET].to_numpy(dtype=np.float32, copy=False)\n",
    "    y_val   = val_small[TARGET].to_numpy(dtype=np.float32, copy=False)\n",
    "    y_test  = test_small[TARGET].to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "    candidate_rows: List[Dict[str, Any]] = []\n",
    "    formulas_concat: List[str] = []\n",
    "    best_idx, best_mse = -1, float(\"inf\")\n",
    "\n",
    "    log(f\"Evaluating {len(candidates)} candidates for year {T} (selection by VALIDATION MSE)...\")\n",
    "    for i, m in enumerate(candidates):\n",
    "        phat_tr = predict_safely(m, train_small, TARGET, feat_cols)\n",
    "        phat_va = predict_safely(m, val_small,   TARGET, feat_cols)\n",
    "        phat_te = predict_safely(m, test_small,  TARGET, feat_cols)\n",
    "\n",
    "        row: Dict[str, Any] = {\"year\": T, \"model_index\": i}\n",
    "        row.update(summary_stats(y_train, phat_tr, \"train\"))\n",
    "        row.update(summary_stats(y_val,   phat_va, \"val\"))     # <-- used for selection\n",
    "        row.update(summary_stats(y_test,  phat_te, \"test\"))\n",
    "\n",
    "        # Persist the model\n",
    "        model_path = os.path.join(year_models_dir, f\"model_{i:03d}.pkl\")\n",
    "        save_pickle(model_path, m)\n",
    "        row[\"model_path\"] = model_path\n",
    "\n",
    "        # Persist formula (symbolic)\n",
    "        form = safe_formula(m)\n",
    "        row[\"formula\"] = form\n",
    "        cand_form_path = os.path.join(year_models_dir, f\"model_{i:03d}_formula.txt\")\n",
    "        save_text(cand_form_path, (form or \"\") + \"\\n\")\n",
    "        formulas_concat.append(f\"### Candidate {i}\\n{form}\\n\")\n",
    "\n",
    "        # Best by validation MSE\n",
    "        val_mse_i = row[\"val_mse\"]\n",
    "        log(f\"[{T}] Model {i}: Val MSE = {val_mse_i:.6g}\")\n",
    "        if np.isfinite(val_mse_i) and (val_mse_i < best_mse):\n",
    "            best_mse, best_idx = val_mse_i, i\n",
    "\n",
    "        candidate_rows.append(row)\n",
    "\n",
    "        # Free per-candidate arrays ASAP\n",
    "        del phat_tr, phat_va, phat_te, form\n",
    "        gc.collect()\n",
    "\n",
    "    if best_idx < 0:\n",
    "        log(f\"Skipping {T}: no finite Val MSE among candidates.\")\n",
    "        del train_small, val_small, test_small, candidates, candidate_rows, formulas_concat\n",
    "        gc.collect()\n",
    "        continue\n",
    "\n",
    "    # flag the best row\n",
    "    for r in candidate_rows:\n",
    "        r[\"is_best\"] = (r[\"model_index\"] == best_idx)\n",
    "        r[\"best_selection_metric\"] = \"val_mse\"\n",
    "\n",
    "    # Save per-year candidate metrics + concatenated formulas\n",
    "    save_json(CANDIDATE_METRICS_JSON, candidate_rows)\n",
    "    save_csv_df(CANDIDATE_METRICS_CSV, pd.DataFrame(candidate_rows))\n",
    "    save_text(ALL_FORMULAS_TXT, \"\\n\".join(formulas_concat))\n",
    "    del candidate_rows, formulas_concat\n",
    "    gc.collect()\n",
    "\n",
    "    # ====== MARK best model clearly inside the candidates folder ======\n",
    "    log(f\"[{T}] Selected best model index {best_idx} by lowest validation MSE = {best_mse:.6g}\")\n",
    "    _mark_best_candidate(year_models_dir, best_idx)\n",
    "\n",
    "    # original file names\n",
    "    best_model_path     = os.path.join(year_models_dir, f\"model_{best_idx:03d}.pkl\")\n",
    "    best_formula_path   = os.path.join(year_models_dir, f\"model_{best_idx:03d}_formula.txt\")\n",
    "\n",
    "    # highlighted copies with BEST__ prefix\n",
    "    best_copy_pkl       = os.path.join(year_models_dir, f\"BEST__model_{best_idx:03d}.pkl\")\n",
    "    best_copy_formula   = os.path.join(year_models_dir, f\"BEST__model_{best_idx:03d}_formula.txt\")\n",
    "    shutil.copyfile(best_model_path,   best_copy_pkl)\n",
    "    shutil.copyfile(best_formula_path, best_copy_formula)\n",
    "    log(f\"[{T}] Created BEST copies: {os.path.basename(best_copy_pkl)}, {os.path.basename(best_copy_formula)}\")\n",
    "\n",
    "    # handy symlinks (fallback to copy if symlinks not allowed)\n",
    "    _try_symlink(best_model_path,   os.path.join(year_models_dir, \"best_model.pkl\"))\n",
    "    _try_symlink(best_formula_path, os.path.join(year_models_dir, \"best_model_formula.txt\"))\n",
    "\n",
    "    # small JSON with the winner’s identity\n",
    "    best_meta = {\n",
    "        \"year\": T,\n",
    "        \"best_index\": int(best_idx),\n",
    "        \"selected_by\": \"val_mse\",\n",
    "        \"val_mse\": float(best_mse),\n",
    "        \"model_path\": best_model_path,\n",
    "        \"formula_path\": best_formula_path,\n",
    "        \"train_years\": [int(y) for y in range(TRAIN_START_YEAR, train_hi + 1)],\n",
    "        \"val_years\": [val_lo, val_hi],\n",
    "        \"test_year\": T,\n",
    "    }\n",
    "    save_json(os.path.join(year_models_dir, f\"best_model_meta_{T}.json\"), best_meta)\n",
    "    del best_meta\n",
    "    gc.collect()\n",
    "\n",
    "    # ====== Save BEST model to per-year artifacts ======\n",
    "    best_model = None\n",
    "    try:\n",
    "        # Reload the best model from disk to avoid holding the whole 'candidates' list in memory\n",
    "        with open(best_model_path, \"rb\") as f:\n",
    "            best_model = pickle.load(f)\n",
    "        log(f\"[{T}] Reloaded BEST model from {best_model_path}\")\n",
    "    except Exception:\n",
    "        # fallback: take from candidates list\n",
    "        best_model = candidates[best_idx]\n",
    "        log(f\"[{T}] Using in-memory BEST model (reload failed or skipped).\")\n",
    "\n",
    "    # free other candidates early\n",
    "    del candidates\n",
    "    gc.collect()\n",
    "\n",
    "    forms = safe_formula(best_model)\n",
    "    save_text(FORMULAS_OUT, (forms or \"\") + \"\\n\")\n",
    "    del forms\n",
    "    gc.collect()\n",
    "\n",
    "    # Persist best model copy in per-year artifacts\n",
    "    save_pickle(MODEL_OUT, best_model)\n",
    "\n",
    "    # Persist preprocessing (float64 stats; feature list)\n",
    "    np.savez_compressed(\n",
    "        PREPROC_OUT,\n",
    "        feat_cols=np.array(feat_cols, dtype=object),\n",
    "        mu=np.asarray(mu, dtype=np.float64),\n",
    "        sd=np.asarray(sd, dtype=np.float64),\n",
    "    )\n",
    "    log(f\"Wrote NPZ: {PREPROC_OUT}  (feat_cols={len(feat_cols)})\")\n",
    "\n",
    "    # Predictions for best model (keep arrays float32)\n",
    "    yhat_train = predict_safely(best_model, train_small, TARGET, feat_cols)\n",
    "    yhat_val   = predict_safely(best_model, val_small,   TARGET, feat_cols)\n",
    "    yhat_test  = predict_safely(best_model, test_small,  TARGET, feat_cols)\n",
    "\n",
    "    best_metrics: Dict[str, Any] = {\"year\": T, \"best_model_index\": int(best_idx), \"best_model_val_mse\": float(best_mse)}\n",
    "    best_metrics.update(summary_stats(y_train, yhat_train, \"train\"))\n",
    "    best_metrics.update(summary_stats(y_val,   yhat_val,   \"val\"))\n",
    "    best_metrics.update(summary_stats(y_test,  yhat_test,  \"test\"))\n",
    "\n",
    "    save_json(METRICS_JSON, best_metrics)\n",
    "    save_csv_df(METRICS_CSV, pd.DataFrame([best_metrics]))\n",
    "\n",
    "    # Dump preds then free arrays quickly\n",
    "    save_csv_df(TRAIN_PRED_CSV, pd.DataFrame({\"y_true\": y_train.astype(np.float32, copy=False),\n",
    "                                              \"y_pred\": yhat_train.astype(np.float32, copy=False)}))\n",
    "    save_csv_df(VAL_PRED_CSV,   pd.DataFrame({\"y_true\": y_val.astype(np.float32, copy=False),\n",
    "                                              \"y_pred\": yhat_val.astype(np.float32, copy=False)}))\n",
    "    save_csv_df(TEST_PRED_CSV,  pd.DataFrame({\"y_true\": y_test.astype(np.float32, copy=False),\n",
    "                                              \"y_pred\": yhat_test.astype(np.float32, copy=False)}))\n",
    "\n",
    "    # Free per-year heavy objects\n",
    "    del y_train, y_val, y_test, yhat_train, yhat_val, yhat_test\n",
    "    del best_model, train_small, val_small, test_small, mu, sd\n",
    "    gc.collect()\n",
    "\n",
    "    # Add to aggregate (keep tiny dict only)\n",
    "    agg_rows.append(best_metrics)\n",
    "    del best_metrics\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "746e9f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:49:19] Wrote CSV: ./qlattice_yearly_summary.csv  (rows=9)\n",
      "[08:49:19] DONE: rolling yearly QLattice training (2017–2025).\n"
     ]
    }
   ],
   "source": [
    "# ============== Aggregate summary across all test years ==============\n",
    "if agg_rows:\n",
    "    save_csv_df(\"./qlattice_yearly_summary.csv\", pd.DataFrame(agg_rows).sort_values(\"year\"))\n",
    "else:\n",
    "    log(\"No years processed successfully; aggregate CSV not created.\")\n",
    "\n",
    "# Final GC\n",
    "gc.collect()\n",
    "log(\"DONE: rolling yearly QLattice training (2017–2025).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db982d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "def load_qlattice_model(model_path: str) -> Any:\n",
    "    \"\"\"\n",
    "    Load a trained QLattice model (.pkl) from disk and return it.\n",
    "    \"\"\"\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_dataframe_per_row(\n",
    "    model: Any,\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    # If you saved qlattice_preproc_<YEAR>.npz, pass it via np.load(..., allow_pickle=True)\n",
    "    # or pass a dict with keys: 'feat_cols', 'mu', 'sd'\n",
    "    preproc: Optional[Dict[str, Any]] = None,\n",
    "    # If you know the feature list explicitly, pass it. Otherwise we infer.\n",
    "    feat_cols: Optional[Sequence[str]] = None,\n",
    "    # Name of target column (if present in df). If provided, we'll try the [y]+X signature first.\n",
    "    yname: Optional[str] = None,\n",
    "    # How to resolve NaNs when preproc is not given:\n",
    "    #   - \"zero\": fill NaNs with 0.0\n",
    "    #   - \"median\": fill NaNs with column medians (computed on df[feat_cols])\n",
    "    impute_strategy: str = \"zero\",\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Run the QLattice model on a DataFrame, returning a 1D Series of predictions\n",
    "    aligned to df.index. We DO NOT drop rows with NaNs. NaNs are imputed.\n",
    "\n",
    "    Priority of feature selection / preprocessing:\n",
    "      1) If `preproc` is provided (with 'feat_cols','mu','sd'), we standardize:\n",
    "         z = (x - mu) / sd   and fill remaining NaNs with 0.\n",
    "      2) Else if `feat_cols` is provided, we use those columns from df\n",
    "         (no standardization), imputing NaNs via `impute_strategy`.\n",
    "      3) Else we infer numeric columns from df (excluding yname if given),\n",
    "         imputing NaNs via `impute_strategy`.\n",
    "\n",
    "    We automatically handle models that expect either X-only or [y]+X DataFrame\n",
    "    ordering (same behavior as in your training code).\n",
    "    \"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(\"df must be a pandas DataFrame\")\n",
    "\n",
    "    # 1) Choose feature columns\n",
    "    if preproc is not None and all(k in preproc for k in (\"feat_cols\", \"mu\", \"sd\")):\n",
    "        # Use training-time feature order\n",
    "        xcols = list(preproc[\"feat_cols\"])\n",
    "        # Intersect with df just in case\n",
    "        xcols = [c for c in xcols if c in df.columns]\n",
    "        if len(xcols) == 0:\n",
    "            raise ValueError(\"No overlap between preproc['feat_cols'] and df columns.\")\n",
    "        X = df[xcols].astype(float, copy=False)\n",
    "        # Standardize using TRAIN stats, then fill remaining NaNs with 0\n",
    "        mu = np.asarray(preproc[\"mu\"], dtype=float)\n",
    "        sd = np.asarray(preproc[\"sd\"], dtype=float)\n",
    "        if mu.shape[0] != len(xcols) or sd.shape[0] != len(xcols):\n",
    "            raise ValueError(\"preproc['mu']/['sd'] length does not match feat_cols.\")\n",
    "\n",
    "        # z-score in a vectorized way\n",
    "        # (broadcast: each column j uses mu[j], sd[j])\n",
    "        Xz = (X - mu) / sd\n",
    "        Xz = Xz.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        # We'll try both signatures below\n",
    "        X_infer = Xz\n",
    "\n",
    "    else:\n",
    "        # No preproc provided; use feat_cols if given, otherwise infer numeric\n",
    "        if feat_cols is not None:\n",
    "            xcols = [c for c in feat_cols if c in df.columns]\n",
    "            if len(xcols) == 0:\n",
    "                raise ValueError(\"Provided feat_cols have no overlap with df columns.\")\n",
    "        else:\n",
    "            # Infer: numeric columns except yname (if present)\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            if yname and yname in numeric_cols:\n",
    "                numeric_cols.remove(yname)\n",
    "            xcols = numeric_cols\n",
    "            if len(xcols) == 0:\n",
    "                raise ValueError(\"Could not infer any numeric feature columns from df.\")\n",
    "\n",
    "        X = df[xcols].astype(float, copy=False)\n",
    "\n",
    "        # Impute NaNs (do NOT drop rows)\n",
    "        if impute_strategy == \"median\":\n",
    "            med = X.median(axis=0, numeric_only=True)\n",
    "            X_infer = X.fillna(med)\n",
    "        else:\n",
    "            # default: zero\n",
    "            X_infer = X.fillna(0.0)\n",
    "\n",
    "        # avoid +/-inf\n",
    "        X_infer = X_infer.replace([np.inf, -np.inf], 0.0)\n",
    "\n",
    "    # 2) Predict using either X-only or [y]+X signature\n",
    "    # We avoid copying the big frame; build small views when needed.\n",
    "    # Keep index alignment by constructing DataFrame views from df.\n",
    "    try:\n",
    "        if yname is not None and yname in df.columns:\n",
    "            df_in = pd.concat([df[[yname]], X_infer], axis=1)\n",
    "            preds = model.predict(df_in)\n",
    "        else:\n",
    "            preds = model.predict(X_infer)\n",
    "    except Exception:\n",
    "        # flip the signature if the first attempt failed\n",
    "        if yname is not None and yname in df.columns:\n",
    "            try:\n",
    "                preds = model.predict(X_infer)\n",
    "            except Exception as e2:\n",
    "                raise RuntimeError(f\"Model prediction failed (both signatures). Last error: {e2}\") from e2\n",
    "        else:\n",
    "            # try [y]+X only if y exists (rare at inference)\n",
    "            if yname is not None and yname in df.columns:\n",
    "                df_in = pd.concat([df[[yname]], X_infer], axis=1)\n",
    "                preds = model.predict(df_in)\n",
    "            else:\n",
    "                raise  # nothing else to try\n",
    "\n",
    "    # Ensure 1D float series aligned to df index\n",
    "    preds = np.asarray(preds, dtype=float).reshape(-1)\n",
    "    if preds.shape[0] != len(df):\n",
    "        raise ValueError(f\"Predictions length {preds.shape[0]} does not match df length {len(df)}.\")\n",
    "    return pd.Series(preds, index=df.index, name=\"y_pred\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ee275a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
