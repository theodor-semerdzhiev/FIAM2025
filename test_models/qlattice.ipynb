{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc2cbbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:42:02] Got 10 candidate models.\n",
      "[19:42:02] Evaluating candidates on VALIDATION and saving each model...\n",
      "[19:42:09]   Model 0: Val MSE = 50.1661\n",
      "[19:42:14]   Model 1: Val MSE = 49.7872\n",
      "[19:42:18]   Model 2: Val MSE = 49.9565\n",
      "[19:42:22]   Model 3: Val MSE = 631.265\n",
      "[19:42:25]   Model 4: Val MSE = 49.7872\n",
      "[19:42:28]   Model 5: Val MSE = 49.7871\n",
      "[19:42:33]   Model 6: Val MSE = 52.844\n",
      "[19:42:36]   Model 7: Val MSE = 1.31187e+19\n",
      "[19:42:39]   Model 8: Val MSE = 49.7872\n",
      "[19:42:43]   Model 9: Val MSE = 49.7871\n",
      "[19:42:43] Selected model index 5 with Val MSE = 49.7871\n",
      "[19:42:43] Saving candidate metrics → qlattice_candidates_metrics.json / qlattice_candidates_metrics.csv\n",
      "[19:42:47] Selected-model metrics:\n",
      "[19:42:47]   train_n: 2912017\n",
      "[19:42:47]   train_mse: 0.27630249556846875\n",
      "[19:42:47]   train_mae: 0.11157694060211236\n",
      "[19:42:47]   train_rmse: 0.5256448378596225\n",
      "[19:42:47]   train_pearson_r: 0.2804722457169072\n",
      "[19:42:47]   train_spearman_ic: 0.0012592760265154105\n",
      "[19:42:47]   train_r2: 0.06881752968909716\n",
      "[19:42:47]   val_n: 604720\n",
      "[19:42:47]   val_mse: 49.78710776812487\n",
      "[19:42:47]   val_mae: 0.11737048924231816\n",
      "[19:42:47]   val_rmse: 7.055997999441671\n",
      "[19:42:47]   val_pearson_r: nan\n",
      "[19:42:47]   val_spearman_ic: nan\n",
      "[19:42:47]   val_r2: -3.591111652490042e-08\n",
      "[19:42:47]   best_model_index: 5\n",
      "[19:42:47]   best_model_val_mse: 49.78710776812487\n",
      "[19:42:47] Top symbolic formula(s) for best model:\n",
      "530.703*exp(-2.0*(0.660793*mispricingperf + 0.293699)**2 - 2.0*(-0.0060505*rdme + (-0.0948954*ivolhxz421d - 1.32439)*(0.00720373*seas25na - 1.80438) + 0.981202)**4) + 0.0205116\n",
      "[19:42:47] Saving BEST model → qlattice_model.pkl\n",
      "[19:42:47] Saving preprocessing → qlattice_preproc.npz\n",
      "[19:42:47] Saving best-model metrics → qlattice_metrics.json / qlattice_metrics.csv\n",
      "[19:42:47] Saving predictions → qlattice_train_predictions.csv / qlattice_val_predictions.csv\n",
      "[19:42:51] DONE: trained on ≤2014; saved ALL candidates; selected by 2015–2016 Val MSE.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# FEYN QLattice — Train on ALL features, select by Val MSE (2015–2016)\n",
    "# Saves ALL candidate models + metrics\n",
    "# ============================================\n",
    "\n",
    "import sys, subprocess, warnings, time, os, pickle, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "import pickle\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "LOG_PATH             = \"feyn_qlattice_train.log\"\n",
    "MODEL_OUT            = \"qlattice_model.pkl\"              # best model\n",
    "PREPROC_OUT          = \"qlattice_preproc.npz\"\n",
    "FORMULAS_OUT         = \"qlattice_formulas.txt\"           # best model's formula\n",
    "METRICS_JSON         = \"qlattice_metrics.json\"           # best model metrics (train/val)\n",
    "METRICS_CSV          = \"qlattice_metrics.csv\"\n",
    "TRAIN_PRED_CSV       = \"qlattice_train_predictions.csv\"  # best model preds\n",
    "VAL_PRED_CSV         = \"qlattice_val_predictions.csv\"    # best model preds\n",
    "\n",
    "# NEW: where to save ALL candidate models + their metrics\n",
    "MODELS_DIR                 = \"qlattice_models\"\n",
    "CANDIDATE_METRICS_JSON     = \"qlattice_candidates_metrics.json\"\n",
    "CANDIDATE_METRICS_CSV      = \"qlattice_candidates_metrics.csv\"\n",
    "\n",
    "# ============== Logging ==============\n",
    "def log(msg: Optional[str] = None) -> None:\n",
    "    if not hasattr(log, \"t0\"):\n",
    "        log.t0 = time.time()\n",
    "        with open(LOG_PATH, \"w\") as f:\n",
    "            f.write(f\"[{time.strftime('%H:%M:%S')}] Log started\\n\")\n",
    "    if msg:\n",
    "        ts = f\"[{time.strftime('%H:%M:%S')}] {msg}\"\n",
    "        print(ts, flush=True)\n",
    "        with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(ts + \"\\n\")\n",
    "\n",
    "# ============== Feyn import ==============\n",
    "log(\"Checking feyn/QLattice...\")\n",
    "try:\n",
    "    from feyn import QLattice\n",
    "    log(\"QLattice import OK.\")\n",
    "except Exception:\n",
    "    log(\"Installing feyn (quiet)...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"feyn\"])\n",
    "    from feyn import QLattice\n",
    "    log(\"QLattice installed & imported.\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ============== Config ==============\n",
    "PATH         = \"../data/ret_sample.parquet\"\n",
    "ID_COL       = \"id\"\n",
    "DATE_COL     = \"date\"\n",
    "TARGET       = \"stock_ret\"\n",
    "RANDOM_SEED  = 42\n",
    "\n",
    "# ============== Load ==============\n",
    "log(f\"Reading parquet: {PATH}\")\n",
    "df = pd.read_parquet(PATH)\n",
    "log(f\"Read done. Rows={len(df):,}, Cols={len(df.columns)}\")\n",
    "\n",
    "# Ensure datetime, year\n",
    "if DATE_COL in df and not np.issubdtype(df[DATE_COL].dtype, np.datetime64):\n",
    "    log(f\"Parsing {DATE_COL} to datetime...\")\n",
    "    df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "if \"year\" not in df.columns:\n",
    "    if DATE_COL in df:\n",
    "        log(\"Deriving 'year' from date column...\")\n",
    "        df[\"year\"] = df[DATE_COL].dt.year\n",
    "    else:\n",
    "        raise RuntimeError(\"No 'year' or parsable DATE_COL found to create splits.\")\n",
    "\n",
    "# ============== Split ==============\n",
    "# Train: year <= 2014 ; Validation: 2015–2016\n",
    "log(\"Creating TRAIN (<=2014) and VAL (2015–2016) splits...\")\n",
    "train_df = df[df[\"year\"] <= 2014]\n",
    "val_df   = df[(df[\"year\"] >= 2015) & (df[\"year\"] <= 2016)]\n",
    "if train_df.empty or val_df.empty:\n",
    "    raise RuntimeError(\"Empty train/val after year split. Check data coverage.\")\n",
    "\n",
    "log(\"Sorting by (id,date)...\")\n",
    "if {ID_COL, DATE_COL} <= set(train_df.columns):\n",
    "    train_df = train_df.sort_values([ID_COL, DATE_COL])\n",
    "if {ID_COL, DATE_COL} <= set(val_df.columns):\n",
    "    val_df = val_df.sort_values([ID_COL, DATE_COL])\n",
    "log(f\"Train rows={len(train_df):,} | Val rows={len(val_df):,}\")\n",
    "\n",
    "# ============== Feature selection (ALL numeric except obvious drops) ==============\n",
    "# (Integrated: keep ALL rows; only drop if TARGET is NaN; NaN-aware standardization; fill NaNs with 0 in z-space.)\n",
    "drop_like = {ID_COL, DATE_COL, TARGET, \"year\", \"month\", \"char_date\", \"gvkey\", \"iid\", \"ret_eom\", \"char_eom\"}\n",
    "\n",
    "num_cols_train: List[str] = [c for c in train_df.select_dtypes(include=[np.number]).columns if c not in drop_like]\n",
    "if not num_cols_train:\n",
    "    raise RuntimeError(\"No numeric candidate features found on TRAIN.\")\n",
    "\n",
    "# Keep only columns available in BOTH train and val\n",
    "num_cols_val_set = set([c for c in val_df.select_dtypes(include=[np.number]).columns if c not in drop_like])\n",
    "feat_cols: List[str] = [c for c in num_cols_train if c in num_cols_val_set]\n",
    "if not feat_cols:\n",
    "    raise RuntimeError(\"No overlapping numeric features between train and val.\")\n",
    "\n",
    "cols = [TARGET] + feat_cols\n",
    "\n",
    "# Build TRAIN/VAL without dropping rows for feature NaNs; drop only rows with NaN TARGET\n",
    "train_small = train_df[cols].copy()\n",
    "val_small   = val_df[cols].copy()\n",
    "\n",
    "train_small = train_small[train_small[TARGET].notna()]\n",
    "val_small   = val_small[val_small[TARGET].notna()]\n",
    "\n",
    "log(f\"train_small (pre-norm) shape={train_small.shape} | val_small (pre-norm) shape={val_small.shape}\")\n",
    "\n",
    "# ============== Standardize using TRAIN stats only (NaN-aware) ==============\n",
    "log(\"Standardizing features with TRAIN stats only (NaN-aware), then imputing remaining NaNs with 0 in z-space...\")\n",
    "mu = np.empty(len(feat_cols), dtype=float)\n",
    "sd = np.empty(len(feat_cols), dtype=float)\n",
    "\n",
    "# Compute stats on TRAIN ignoring NaNs\n",
    "for j, c in enumerate(feat_cols):\n",
    "    arr = train_small[c].to_numpy(dtype=float, copy=False)\n",
    "    mu[j] = np.nanmean(arr)\n",
    "    sd[j] = np.nanstd(arr, ddof=1)\n",
    "\n",
    "# avoid division by zero / non-finite\n",
    "sd[~np.isfinite(sd)] = 1.0\n",
    "sd[sd == 0.0] = 1.0\n",
    "\n",
    "# Apply z-score transform, keep NaNs for now\n",
    "for j, c in enumerate(feat_cols):\n",
    "    t = train_small[c].to_numpy(dtype=float, copy=False)\n",
    "    v = val_small[c].to_numpy(dtype=float, copy=False)\n",
    "    train_small[c] = (t - mu[j]) / sd[j]\n",
    "    val_small[c]   = (v - mu[j]) / sd[j]\n",
    "\n",
    "# Fill remaining NaNs with 0.0 AFTER standardization (mean-imputation in z-space)\n",
    "train_small[feat_cols] = train_small[feat_cols].fillna(0.0)\n",
    "val_small[feat_cols]   = val_small[feat_cols].fillna(0.0)\n",
    "\n",
    "log(\"Standardization + NaN-imputation complete.\")\n",
    "log(f\"train_small (post-norm) shape={train_small.shape} | val_small (post-norm) shape={val_small.shape}\")\n",
    "\n",
    "# ============== Train candidate models ==============\n",
    "def _make_ql() -> QLattice:\n",
    "    try:\n",
    "        return QLattice(random_seed=RANDOM_SEED)\n",
    "    except TypeError:\n",
    "        try:\n",
    "            return QLattice(random_state=RANDOM_SEED)\n",
    "        except TypeError:\n",
    "            return QLattice()\n",
    "\n",
    "def fit_candidates(df_small: pd.DataFrame, yname: str):\n",
    "    ql = _make_ql()\n",
    "    log(\"QLattice.auto_run(...)\")\n",
    "    models = ql.auto_run(df_small, yname, max_complexity=25, n_epochs=18, criterion=\"bic\")\n",
    "    log(f\"Got {len(models)} candidate models.\")\n",
    "    return models\n",
    "\n",
    "def predict_safely(model: Any, df_small: pd.DataFrame, yname: str, xcols: List[str]) -> np.ndarray:\n",
    "    try:\n",
    "        return np.asarray(model.predict(df_small[[yname] + xcols]), dtype=float)\n",
    "    except Exception:\n",
    "        return np.asarray(model.predict(df_small[xcols]), dtype=float)\n",
    "\n",
    "def mse(y: np.ndarray, p: np.ndarray) -> float:\n",
    "    return float(np.nanmean((p - y)**2))\n",
    "\n",
    "def summary_stats(y: np.ndarray, p: np.ndarray, prefix: str) -> Dict[str, Any]:\n",
    "    resid = p - y\n",
    "    out: Dict[str, Any] = {\n",
    "        f\"{prefix}_n\": int(np.isfinite(y).sum()),\n",
    "        f\"{prefix}_mse\": float(np.nanmean(resid**2)),\n",
    "        f\"{prefix}_mae\": float(np.nanmean(np.abs(resid))),\n",
    "        f\"{prefix}_rmse\": float(np.sqrt(np.nanmean(resid**2))),\n",
    "    }\n",
    "    # Pearson r\n",
    "    try:\n",
    "        r, _ = pearsonr(y, p)\n",
    "        out[f\"{prefix}_pearson_r\"] = float(r)\n",
    "    except Exception:\n",
    "        out[f\"{prefix}_pearson_r\"] = float(\"nan\")\n",
    "    # Spearman\n",
    "    try:\n",
    "        ic = spearmanr(y, p, nan_policy=\"omit\").correlation\n",
    "        out[f\"{prefix}_spearman_ic\"] = float(ic) if ic is not None else float(\"nan\")\n",
    "    except Exception:\n",
    "        out[f\"{prefix}_spearman_ic\"] = float(\"nan\")\n",
    "    # R^2\n",
    "    ybar = float(np.nanmean(y))\n",
    "    ss_tot = float(np.nansum((y - ybar)**2))\n",
    "    ss_res = float(np.nansum((y - p )**2))\n",
    "    out[f\"{prefix}_r2\"] = float(\"nan\") if ss_tot == 0 else 1.0 - (ss_res / ss_tot)\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_qlattice_model(\n",
    "    model_path: str,\n",
    "    preproc_path: Optional[str] = None\n",
    ") -> Tuple[Any, Optional[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Load a trained QLattice model and optional preprocessing metadata.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the pickled QLattice model file (.pkl).\n",
    "        preproc_path (Optional[str]): Path to the preprocessing .npz file\n",
    "            that contains 'feat_cols', 'mu', and 'sd'. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Any, Optional[Dict[str, Any]]]:\n",
    "            - model: the unpickled QLattice model object.\n",
    "            - preproc: a dictionary with preprocessing info\n",
    "              (keys: 'feat_cols', 'mu', 'sd') if provided, else None.\n",
    "    \"\"\"\n",
    "    # ---- Load model ----\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "\n",
    "    preproc: Optional[Dict[str, Any]] = None\n",
    "\n",
    "    # ---- Load preprocessing (optional) ----\n",
    "    if preproc_path is not None:\n",
    "        data = np.load(preproc_path, allow_pickle=True)\n",
    "        preproc = {\n",
    "            \"feat_cols\": data[\"feat_cols\"].tolist(),\n",
    "            \"mu\": data[\"mu\"],\n",
    "            \"sd\": data[\"sd\"],\n",
    "        }\n",
    "\n",
    "    return model, preproc\n",
    "\n",
    "log(\"Fitting candidates on TRAIN...\")\n",
    "candidates = fit_candidates(train_small, TARGET)\n",
    "if not candidates:\n",
    "    raise RuntimeError(\"No models returned by QLattice.auto_run\")\n",
    "\n",
    "# Ensure models dir exists\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# ============== Evaluate ALL models; save each; pick best by Val MSE ==============\n",
    "y_train = train_small[TARGET].to_numpy(dtype=float, copy=False)\n",
    "y_val   = val_small[TARGET].to_numpy(dtype=float, copy=False)\n",
    "\n",
    "candidate_rows: List[Dict[str, Any]] = []\n",
    "best_idx, best_mse = -1, float(\"inf\")\n",
    "\n",
    "log(\"Evaluating candidates on VALIDATION and saving each model...\")\n",
    "for i, m in enumerate(candidates):\n",
    "    # Predictions\n",
    "    phat_tr = predict_safely(m, train_small, TARGET, feat_cols)\n",
    "    phat_va = predict_safely(m, val_small,   TARGET, feat_cols)\n",
    "\n",
    "    # Metrics\n",
    "    row: Dict[str, Any] = {\"model_index\": i}\n",
    "    row.update(summary_stats(y_train, phat_tr, \"train\"))\n",
    "    row.update(summary_stats(y_val,   phat_va, \"val\"))\n",
    "\n",
    "    # Persist the model\n",
    "    model_path = os.path.join(MODELS_DIR, f\"model_{i:03d}.pkl\")\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(m, f)\n",
    "    row[\"model_path\"] = model_path\n",
    "\n",
    "    # Persist its formula (if available)\n",
    "    try:\n",
    "        form = str(m.sympify())\n",
    "    except Exception:\n",
    "        form = \"\"\n",
    "    row[\"formula\"] = form\n",
    "    if form:\n",
    "        with open(os.path.join(MODELS_DIR, f\"model_{i:03d}_formula.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(form + \"\\n\")\n",
    "\n",
    "    # Track best by validation MSE\n",
    "    val_mse_i = row[\"val_mse\"]\n",
    "    log(f\"  Model {i}: Val MSE = {val_mse_i:.6g}\")\n",
    "    if np.isfinite(val_mse_i) and (val_mse_i < best_mse):\n",
    "        best_mse, best_idx = val_mse_i, i\n",
    "\n",
    "    candidate_rows.append(row)\n",
    "\n",
    "if best_idx < 0:\n",
    "    raise RuntimeError(\"Could not select a best model (no finite Val MSE).\")\n",
    "best_model = candidates[best_idx]\n",
    "log(f\"Selected model index {best_idx} with Val MSE = {best_mse:.6g}\")\n",
    "\n",
    "# Save candidate metrics table\n",
    "log(f\"Saving candidate metrics → {CANDIDATE_METRICS_JSON} / {CANDIDATE_METRICS_CSV}\")\n",
    "with open(CANDIDATE_METRICS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(candidate_rows, f, indent=2)\n",
    "pd.DataFrame(candidate_rows).to_csv(CANDIDATE_METRICS_CSV, index=False)\n",
    "\n",
    "# ============== Save best-model artifacts (metrics + preds + preproc) ==============\n",
    "yhat_train = predict_safely(best_model, train_small, TARGET, feat_cols)\n",
    "yhat_val   = predict_safely(best_model, val_small,   TARGET, feat_cols)\n",
    "\n",
    "best_metrics: Dict[str, Any] = {}\n",
    "best_metrics.update(summary_stats(y_train, yhat_train, \"train\"))\n",
    "best_metrics.update(summary_stats(y_val,   yhat_val,   \"val\"))\n",
    "best_metrics[\"best_model_index\"]   = int(best_idx)\n",
    "best_metrics[\"best_model_val_mse\"] = float(best_mse)\n",
    "\n",
    "log(\"Selected-model metrics:\")\n",
    "for k, v in best_metrics.items():\n",
    "    log(f\"  {k}: {v}\")\n",
    "\n",
    "# Save best model formula (also kept per-model above)\n",
    "try:\n",
    "    forms = str(best_model.sympify())\n",
    "    log(\"Top symbolic formula(s) for best model:\")\n",
    "    print(forms)\n",
    "    with open(FORMULAS_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(forms + \"\\n\")\n",
    "except Exception as e:\n",
    "    log(f\"Could not extract symbolic formulas: {e}\")\n",
    "\n",
    "# Save best model\n",
    "log(f\"Saving BEST model → {MODEL_OUT}\")\n",
    "with open(MODEL_OUT, \"wb\") as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "# Save preprocessing for inference\n",
    "log(f\"Saving preprocessing → {PREPROC_OUT}\")\n",
    "np.savez_compressed(\n",
    "    PREPROC_OUT,\n",
    "    feat_cols=np.array(feat_cols, dtype=object),\n",
    "    mu=mu.astype(np.float64),\n",
    "    sd=sd.astype(np.float64),\n",
    ")\n",
    "\n",
    "# Save best model metrics and predictions\n",
    "log(f\"Saving best-model metrics → {METRICS_JSON} / {METRICS_CSV}\")\n",
    "with open(METRICS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best_metrics, f, indent=2)\n",
    "pd.DataFrame([best_metrics]).to_csv(METRICS_CSV, index=False)\n",
    "\n",
    "log(f\"Saving predictions → {TRAIN_PRED_CSV} / {VAL_PRED_CSV}\")\n",
    "pd.DataFrame({\"y_true\": y_train, \"y_pred\": yhat_train}).to_csv(TRAIN_PRED_CSV, index=False)\n",
    "pd.DataFrame({\"y_true\": y_val,   \"y_pred\": yhat_val  }).to_csv(VAL_PRED_CSV,   index=False)\n",
    "\n",
    "log(\"DONE: trained on ≤2014; saved ALL candidates; selected by 2015–2016 Val MSE.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79e3b7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'date', 'ret_eom', 'gvkey', 'iid', 'excntry', 'stock_ret', 'year',\n",
      "       'month', 'char_date',\n",
      "       ...\n",
      "       'betadown_252d', 'prc_highprc_252d', 'corr_1260d', 'betabab_1260d',\n",
      "       'rmax5_rvol_21d', 'age', 'qmj', 'qmj_prof', 'qmj_growth', 'qmj_safety'],\n",
      "      dtype='object', length=159)\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb1b8617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from typing import Any, Dict, Tuple, Optional\n",
    "\n",
    "def load_qlattice_model(\n",
    "    model_path: str,\n",
    "    preproc_path: Optional[str] = None\n",
    ") -> Tuple[Any, Optional[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Load a trained QLattice model and optional preprocessing metadata.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the pickled QLattice model file (.pkl).\n",
    "        preproc_path (Optional[str]): Path to the preprocessing .npz file\n",
    "            that contains 'feat_cols', 'mu', and 'sd'. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Any, Optional[Dict[str, Any]]]:\n",
    "            - model: the unpickled QLattice model object.\n",
    "            - preproc: a dictionary with preprocessing info\n",
    "              (keys: 'feat_cols', 'mu', 'sd') if provided, else None.\n",
    "    \"\"\"\n",
    "    # ---- Load model ----\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "\n",
    "    preproc: Optional[Dict[str, Any]] = None\n",
    "\n",
    "    # ---- Load preprocessing (optional) ----\n",
    "    if preproc_path is not None:\n",
    "        data = np.load(preproc_path, allow_pickle=True)\n",
    "        preproc = {\n",
    "            \"feat_cols\": data[\"feat_cols\"].tolist(),\n",
    "            \"mu\": data[\"mu\"],\n",
    "            \"sd\": data[\"sd\"],\n",
    "        }\n",
    "\n",
    "    return model, preproc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae631feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preproc = load_qlattice_model(\n",
    "    model_path=\"/Users/tsemerdz/Projects/FIAM2025/test_models/qlattice_model.pkl\",\n",
    "    preproc_path=\"/Users/tsemerdz/Projects/FIAM2025/test_models/qlattice_preproc.npz\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "556c5f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.929863*exp((3.02132 - 0.0149948*ocfme)*(0.0576986*beme - 2.35076)) + 0.00613281\n"
     ]
    }
   ],
   "source": [
    "print(model.sympify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0483ef86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 530.703 e^{- 2.0 \\left(0.660793 mispricingperf + 0.293699\\right)^{2} - 2.0 \\left(- 0.0060505 rdme + \\left(- 0.0948954 ivolhxz421d - 1.32439\\right) \\left(0.00720373 seas25na - 1.80438\\right) + 0.981202\\right)^{4}} + 0.0205116$"
      ],
      "text/plain": [
       "530.703*exp(-2.0*(0.660793*mispricingperf + 0.293699)**2 - 2.0*(-0.0060505*rdme + (-0.0948954*ivolhxz421d - 1.32439)*(0.00720373*seas25na - 1.80438) + 0.981202)**4) + 0.0205116"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sympify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17739c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sympify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e88fd306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Any, Dict, Optional, Sequence\n",
    "\n",
    "def predict_with_qlattice(\n",
    "    model: Any,\n",
    "    df: pd.DataFrame,\n",
    "    preproc: Optional[Dict[str, Any]] = None,\n",
    "    target_col: Optional[str] = None,\n",
    "    feat_cols: Optional[Sequence[str]] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Robust QLattice inference:\n",
    "      - Ensures the exact feature columns (from `preproc['feat_cols']` or `feat_cols`).\n",
    "      - Standardizes using `preproc['mu']`/`preproc['sd']` if provided (leak-safe).\n",
    "      - Replaces any NaN/±Inf in inputs with 0.0.\n",
    "      - Tries both input layouts that QLattice models accept.\n",
    "\n",
    "    Args:\n",
    "      model: unpickled QLattice model.\n",
    "      df:    DataFrame with raw features.\n",
    "      preproc: optional dict with keys 'feat_cols', 'mu', 'sd'.\n",
    "      target_col: if your model expects [y, X...] format, pass the target name (e.g. \"stock_ret\").\n",
    "                  We'll add a dummy zero column if it's not present.\n",
    "      feat_cols: override list of feature names (used only if preproc is None).\n",
    "\n",
    "    Returns:\n",
    "      np.ndarray of predictions (float64).\n",
    "    \"\"\"\n",
    "    # ---- Decide feature set ----\n",
    "    if preproc is not None:\n",
    "        feat_cols_from = list(preproc[\"feat_cols\"])\n",
    "        mu = np.asarray(preproc[\"mu\"], dtype=float)\n",
    "        sd = np.asarray(preproc[\"sd\"], dtype=float)\n",
    "        if len(feat_cols_from) != len(mu) or len(mu) != len(sd):\n",
    "            raise ValueError(\"preproc feat_cols/mu/sd length mismatch.\")\n",
    "        use_cols = feat_cols_from\n",
    "        # fix degenerate or non-finite sd/mu\n",
    "        sd = np.where(~np.isfinite(sd) | (sd == 0.0), 1.0, sd)\n",
    "        mu = np.where(~np.isfinite(mu), 0.0, mu)\n",
    "        mu_s = pd.Series(mu, index=use_cols)\n",
    "        sd_s = pd.Series(sd, index=use_cols)\n",
    "    else:\n",
    "        # No preproc: fall back to user-supplied feat_cols or numeric intersection\n",
    "        if feat_cols is not None:\n",
    "            use_cols = list(feat_cols)\n",
    "        else:\n",
    "            use_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        mu_s = None\n",
    "        sd_s = None\n",
    "\n",
    "    # ---- Build X with exact columns & sanitize ----\n",
    "    # Reindex to require the exact features; fill missing with 0 before scaling\n",
    "    X = df.reindex(columns=use_cols, fill_value=0.0).astype(float, copy=False)\n",
    "\n",
    "    # Standardize if we have training stats\n",
    "    if mu_s is not None and sd_s is not None:\n",
    "        # vectorized, index-aligned ops\n",
    "        X = (X - mu_s) / sd_s\n",
    "\n",
    "    # Replace any residual non-finite values with 0\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    # If model expects [y] + X, make sure the y column exists (zeros; not used at inference)\n",
    "    if target_col:\n",
    "        if target_col not in X.columns:\n",
    "            X = pd.concat([pd.DataFrame({target_col: np.zeros(len(X), dtype=float)}, index=X.index), X], axis=1)\n",
    "\n",
    "    # ---- Predict robustly ----\n",
    "    try:\n",
    "        # Try with current column order\n",
    "        preds = np.asarray(model.predict(X), dtype=float)\n",
    "    except Exception:\n",
    "        # If we inserted a target_col, ensure it is first\n",
    "        if target_col and target_col in X.columns and X.columns[0] != target_col:\n",
    "            order = [target_col] + [c for c in X.columns if c != target_col]\n",
    "            preds = np.asarray(model.predict(X[order]), dtype=float)\n",
    "        else:\n",
    "            # As a last resort, try without target if present\n",
    "            if target_col and target_col in X.columns:\n",
    "                X_wo_y = X[[c for c in X.columns if c != target_col]]\n",
    "                preds = np.asarray(model.predict(X_wo_y), dtype=float)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    # Final sanitize of outputs\n",
    "    preds = np.where(np.isfinite(preds), preds, 0.0)\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64c703e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This version of Feyn and the QLattice is available for academic, personal, and non-commercial use. By using the community version of this software you agree to the terms and conditions which can be found at https://abzu.ai/eula."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model + preprocessing\n",
    "model, preproc = load_qlattice_model(\"qlattice_model.pkl\", \"qlattice_preproc.npz\")\n",
    "PATH = \"../data/ret_sample.parquet\"\n",
    "df = pd.read_parquet(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b31d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions\n",
    "y_pred = predict_with_qlattice(model, df[df[\"year\"] >= 2017], preproc, target_col=\"stock_ret\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04c794a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.020511564125235738)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_pred[2451122]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
