{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fa87b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85c0aaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carte\\AppData\\Local\\Temp\\ipykernel_29620\\2979320123.py:3: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(ret_sample_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'date', 'ret_eom', 'gvkey', 'iid', 'excntry', 'stock_ret', 'year', 'month', 'char_date',\n",
      "       ...\n",
      "       'betadown_252d', 'prc_highprc_252d', 'corr_1260d', 'betabab_1260d', 'rmax5_rvol_21d', 'age', 'qmj', 'qmj_prof', 'qmj_growth', 'qmj_safety'], dtype='object', length=159)\n"
     ]
    }
   ],
   "source": [
    "ret_sample_path = r'C:\\_Files\\Personal\\Projects\\FIAM\\FIAM2025\\data\\ret_sample.csv'\n",
    "\n",
    "df = pd.read_csv(ret_sample_path)\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c98591b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date\n",
      "1695191  20101130\n",
      "5021799  20210831\n",
      "1109133  20081128\n",
      "3941632  20180531\n",
      "2214206  20120831\n",
      "677865   20070629\n",
      "3413785  20160831\n",
      "4202991  20190329\n",
      "405709   20060721\n",
      "4193811  20190228\n",
      "1377171  20091029\n",
      "3649416  20170630\n",
      "6369671  20250530\n",
      "83198    20050531\n",
      "3856221  20180228\n",
      "449196   20060929\n",
      "2960688  20150227\n",
      "5884210  20240131\n",
      "4654061  20200731\n",
      "4325633  20190731\n",
      "6295073  20250331\n",
      "3201820  20151230\n",
      "148835   20050831\n",
      "2303098  20121228\n",
      "1256409  20090529\n",
      "4453515  20191231\n",
      "1898712  20110729\n",
      "621200   20070430\n",
      "4542759  20200331\n",
      "4155784  20190130\n",
      "447084   20060929\n",
      "5997297  20240531\n",
      "104683   20050630\n",
      "5108882  20211130\n",
      "3042563  20150630\n",
      "1326575  20090831\n",
      "1114031  20081230\n",
      "6386094  20250630\n",
      "2774267  20140731\n",
      "3925624  20180430\n",
      "1761546  20110228\n",
      "1014558  20080829\n",
      "452082   20060928\n",
      "1121113  20081230\n",
      "1415676  20091230\n",
      "195749   20051031\n",
      "5188294  20220228\n",
      "5629329  20230428\n",
      "2481787  20130731\n",
      "975004   20080630\n",
      "2273765  20121129\n",
      "5051515  20210930\n",
      "2377526  20130329\n",
      "4369177  20190930\n",
      "461199   20060929\n",
      "447251   20060929\n",
      "4127906  20181231\n",
      "1600034  20100730\n",
      "727386   20070831\n",
      "3825386  20171229\n",
      "2994432  20150430\n",
      "1759391  20110228\n",
      "1856421  20110630\n",
      "5122636  20211230\n",
      "2084800  20120330\n",
      "3251114  20160229\n",
      "3647586  20170630\n",
      "5410970  20220929\n",
      "3683149  20170707\n",
      "1095537  20081128\n",
      "677369   20070629\n",
      "3847603  20180131\n",
      "1040790  20080930\n",
      "6168452  20241031\n",
      "2954689  20150227\n",
      "5667104  20230630\n",
      "5979977  20240430\n",
      "4379170  20190930\n",
      "4993375  20210730\n",
      "6198127  20241129\n",
      "6224854  20241231\n",
      "256473   20060131\n",
      "113831   20050729\n",
      "2989328  20150430\n",
      "4077127  20181031\n",
      "1660009  20101029\n",
      "2870292  20141128\n",
      "742074   20070928\n",
      "2140349  20120531\n",
      "4225011  20190329\n",
      "455162   20060929\n",
      "5030799  20210930\n",
      "202370   20051031\n",
      "66915    20050429\n",
      "5619750  20230428\n",
      "13943    20050216\n",
      "3946840  20180531\n",
      "3870724  20180228\n",
      "1897380  20110729\n",
      "1297119  20090731\n"
     ]
    }
   ],
   "source": [
    "ret_sample_path = r'C:\\_Files\\Personal\\Projects\\FIAM\\FIAM2025\\data\\ret_sample.csv'\n",
    "pd.set_option(\"display.max_rows\", 1000)\n",
    "df = pd.read_csv(ret_sample_path, usecols=['date'])\n",
    "print(df.sample(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80e73c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the memory-efficient script to split data by company ID.\n",
      "Created output directory: C:\\_Files\\Personal\\Projects\\FIAM\\FIAM2025\\data\\csv_by_id\n",
      "Reading data from C:\\_Files\\Personal\\Projects\\FIAM\\FIAM2025\\data\\ret_sample.csv in chunks of 1000000 rows...\n",
      "--- Processing Chunk 1 ---\n",
      "Cleaning 'gvkey' column and preparing for grouping...\n",
      "Grouping by 'gvkey' and saving to files...\n",
      "Chunk 1 processed successfully.\n",
      "Unique IDs found so far: 29651\n",
      "--- Processing Chunk 2 ---\n",
      "Cleaning 'gvkey' column and preparing for grouping...\n",
      "Grouping by 'gvkey' and saving to files...\n",
      "Chunk 2 processed successfully.\n",
      "Unique IDs found so far: 33335\n",
      "--- Processing Chunk 3 ---\n",
      "Cleaning 'gvkey' column and preparing for grouping...\n",
      "Grouping by 'gvkey' and saving to files...\n",
      "Chunk 3 processed successfully.\n",
      "Unique IDs found so far: 36931\n",
      "--- Processing Chunk 4 ---\n",
      "Cleaning 'gvkey' column and preparing for grouping...\n",
      "Grouping by 'gvkey' and saving to files...\n",
      "Chunk 4 processed successfully.\n",
      "Unique IDs found so far: 41556\n",
      "--- Processing Chunk 5 ---\n",
      "Cleaning 'gvkey' column and preparing for grouping...\n",
      "Grouping by 'gvkey' and saving to files...\n",
      "Chunk 5 processed successfully.\n",
      "Unique IDs found so far: 46895\n",
      "--- Processing Chunk 6 ---\n",
      "Cleaning 'gvkey' column and preparing for grouping...\n",
      "Grouping by 'gvkey' and saving to files...\n",
      "Chunk 6 processed successfully.\n",
      "Unique IDs found so far: 51042\n",
      "--- Processing Chunk 7 ---\n",
      "Cleaning 'gvkey' column and preparing for grouping...\n",
      "Grouping by 'gvkey' and saving to files...\n",
      "Chunk 7 processed successfully.\n",
      "Unique IDs found so far: 52268\n",
      "\n",
      "Processing complete. All chunks have been processed and files saved.\n",
      "Total unique IDs found and files created: 52268\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def process_csv_in_chunks(input_csv_path, output_dir, chunk_size=1000000):\n",
    "    \"\"\"\n",
    "    Reads and processes a large CSV in chunks to avoid memory errors. It\n",
    "    uses the 'gvkey' column to save separate CSV files for each unique ID.\n",
    "\n",
    "    Args:\n",
    "        input_csv_path (str): The path to the input CSV file.\n",
    "        output_dir (str): The directory where the output CSVs will be saved.\n",
    "        chunk_size (int): The number of rows to process in each chunk.\n",
    "    \"\"\"\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Created output directory: {output_dir}\")\n",
    "\n",
    "    # This set will keep track of which files we have already written the header for\n",
    "    created_files = set()\n",
    "    \n",
    "    try:\n",
    "        print(f\"Reading data from {input_csv_path} in chunks of {chunk_size} rows...\")\n",
    "        \n",
    "        # Use the 'chunksize' parameter to create an iterator\n",
    "        chunk_iterator = pd.read_csv(input_csv_path, low_memory=False, chunksize=chunk_size)\n",
    "        \n",
    "        # Loop through each chunk (a smaller DataFrame) from the file\n",
    "        for i, chunk in enumerate(chunk_iterator):\n",
    "            print(f\"--- Processing Chunk {i+1} ---\")\n",
    "\n",
    "            # --- ID Transformation using 'gvkey' column ---\n",
    "            print(\"Cleaning 'gvkey' column and preparing for grouping...\")\n",
    "            # Drop rows where gvkey is missing\n",
    "            chunk.dropna(subset=['gvkey'], inplace=True)\n",
    "            # Convert gvkey to integer type to remove decimals (e.g., 1081.0 -> 1081)\n",
    "            chunk['gvkey'] = chunk['gvkey'].astype(int)\n",
    "            \n",
    "            # --- Append rows to the correct files ---\n",
    "            print(\"Grouping by 'gvkey' and saving to files...\")\n",
    "            # Group the current chunk by the integer 'gvkey'\n",
    "            grouped_chunk = chunk.groupby('gvkey')\n",
    "            \n",
    "            # Loop through each unique ID found in this chunk\n",
    "            for current_id, group_df in grouped_chunk:\n",
    "                file_name = f\"{current_id}.csv\"\n",
    "                output_path = os.path.join(output_dir, file_name)\n",
    "                \n",
    "                # If we haven't created this file yet, write with a header.\n",
    "                # Otherwise, append without the header.\n",
    "                if output_path not in created_files:\n",
    "                    group_df.to_csv(output_path, index=False, mode='w', header=True)\n",
    "                    created_files.add(output_path) # Mark this file as created\n",
    "                else:\n",
    "                    group_df.to_csv(output_path, index=False, mode='a', header=False)\n",
    "            \n",
    "            print(f\"Chunk {i+1} processed successfully.\")\n",
    "            # Add a running counter after each chunk is processed\n",
    "            print(f\"Unique IDs found so far: {len(created_files)}\")\n",
    "\n",
    "        print(\"\\nProcessing complete. All chunks have been processed and files saved.\")\n",
    "        # Add a final count at the end of the script\n",
    "        print(f\"Total unique IDs found and files created: {len(created_files)}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{input_csv_path}' was not found. Please check the path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# --- How to use this script ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting the memory-efficient script to split data by company ID.\")\n",
    "    \n",
    "    # 1. Set the path to your input CSV file.\n",
    "    input_file_path = r'C:\\_Files\\Personal\\Projects\\FIAM\\FIAM2025\\data\\ret_sample.csv'\n",
    "    \n",
    "    # 2. Set the path for the output folder.\n",
    "    output_folder_name = r'C:\\_Files\\Personal\\Projects\\FIAM\\FIAM2025\\data\\csv_by_id'\n",
    "    \n",
    "    # 3. Run the main function.\n",
    "    process_csv_in_chunks(input_file_path, output_folder_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b436e8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of             date      cik file_type                                                                                                                                                                                                       rf  \\\n",
      "177795  20050103    16099       10Q                                                                                                                                                                                                            \n",
      "177791  20050103   779544       10K                                                                                                                                                                                                            \n",
      "177794  20050103   831641       10K                                                                                                                                                                                                            \n",
      "177790  20050103   866415       10K                                                                                                                                                                                                            \n",
      "177793  20050103  1141240       10Q                                                                                                                                                                                                            \n",
      "...          ...      ...       ...                                                                                                                                                                                                      ...   \n",
      "195708  20051229  1100983       10K                                                                                                                                                                                                            \n",
      "195726  20051229  1122668       10K  ITEM 1A. RISK FACTORS\\n\\n This Report contains forward-looking statements based on the current\\nexpectations, assumptions, estimates and projections about us and our industry.\\nOur actual results ...   \n",
      "195718  20051229  1310094       10K                                                                                                                                                                                                            \n",
      "195714  20051229  1311396       10K  Item 1A. \\n\\nRisk Factors ITEM 1A. Risk Factors \\n\\nAviza\\nwill need to raise capital in order to support its operations, which capital\\nmay not be available on terms acceptable to Aviza, or at al...   \n",
      "195737  20051230    16099       10Q                                                                                                                                                                                                            \n",
      "\n",
      "                                                                                                                                                                                                           mgmt     gvkey      cusip  year  \n",
      "177795  Item 2 Management s Discussion and Analysis of Financial Condition and Results of Operations \\n\\n18 Item 2. Management s Discussion and Analysis of Financial Condition of\\nOperations \\n\\nManagemen...    6831.0  549282101  2005  \n",
      "177791  Item 7. Management's Discussion and Analysis of Financial Condition and Results\\n of Operations\\n\\nAccounting period\\n\\nThe Company's fiscal year ends on the Saturday nearest September 30. The Com...   11872.0  040712101  2005  \n",
      "177794  Item 7 \\n \\n Management's Discussion and Analysis of Financial Condition and Results of Operations \\n \\n 30 Item 7. Management's Discussion and Analysis of Financial Condition and Results of Opera...   24783.0  88162G103  2005  \n",
      "177790  ITEM 7. Management's Discussion and Analysis of Financial\\n Condition and Results of Operations 17\\n\\nRISK FACTORS Item 7\\n- - Management's Discussion and Analysis of Financial Condition and Resul...   61721.0  459412102  2005  \n",
      "177793  Item\\n 2 Management s Discussion and Analysis of Financial Condition and Results\\n of Operations Item 2. Management s Discussion and Analysis of\\nFinancial Condition and Results of Operations \\n\\n...  146117.0  53634X100  2005  \n",
      "...                                                                                                                                                                                                         ...       ...        ...   ...  \n",
      "195708  Item 7. Management s Discussion and Analysis\\nof Financial Condition and Results of Operations \\n\\nThe information required herein is\\nincorporated by reference from pages 6 to16 of the 2005 Annua...  133506.0  71086E107  2005  \n",
      "195726  ITEM 7. MANAGEMENT'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION\\n AND RESULTS OF OPERATIONS...................................33 ITEM 7. MANAGEMENT'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDI...  141007.0  68382T101  2005  \n",
      "195718  ITEM 7. MANAGEMENT S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS When you read this section of this Form 10K, it is important that you also read the\\nfinancial stateme...  162956.0  00430L103  2005  \n",
      "195714  Item\\n 7. \\n\\nManagement s\\n Discussion and Analysis of Financial Condition and Results of Operations Item 7. Management s Discussion and Analysis of\\nFinancial Condition and Results of \\n\\n23 \\n\\...  165666.0  05381A105  2005  \n",
      "195737  Item 2. \\n\\nManagement s\\n Discussion and Analysis of Financial\\n Condition and Results of\\n Operations \\n\\nManagement's\\n discussion and analysis of financial condition and results of operations\\...    6831.0  549282101  2005  \n",
      "\n",
      "[16857 rows x 8 columns]>\n",
      "Index(['date', 'cik', 'file_type', 'rf', 'mgmt', 'gvkey', 'cusip', 'year'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# rf and mgmnt are the risk factors and management discussion and analysis respectively. \n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# 2. Set display width: This is the key setting to prevent line wrapping.\n",
    "#    We set it to a very large number to ensure pandas tries to fit everything on one line.\n",
    "pd.set_option('display.width', 300) # You can increase this number if needed\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "# Optional: If you also want to see more rows in your output\n",
    "pd.set_option('display.max_rows', 100) # This will show up to 100 rows\n",
    "\n",
    "directory_path = r'C:\\_Files\\Personal\\Projects\\FIAM\\FIAM2025\\data\\text_data'\n",
    "\n",
    "pickle_pattern = os.path.join(directory_path, '**', '*.pkl')\n",
    "all_pickle_files = glob.glob(pickle_pattern, recursive=True)\n",
    "\n",
    "if all_pickle_files:\n",
    "\n",
    "    first_file_path = all_pickle_files[0]\n",
    "\n",
    "    df = pd.read_pickle(first_file_path)\n",
    "\n",
    "    print(df.head)\n",
    "    print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b300c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\_Files\\Personal\\Projects\\FIAM\\FIAM2025\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model 'ProsusAI/finbert'... (This may take a moment on first run)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\_Files\\Personal\\Projects\\FIAM\\FIAM2025\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\carte\\.cache\\huggingface\\hub\\models--ProsusAI--finbert. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "\n",
      "--- FinBERT Sentiment Analysis Results ---\n",
      "  Sentence:   'The company's earnings exceeded expectations, driven by strong sales growth.'\n",
      "  Sentiment:  Positive (Confidence: 95.96%)\n",
      "\n",
      "  Sentence:   'Regulatory concerns and rising costs have negatively impacted our outlook.'\n",
      "  Sentiment:  Negative (Confidence: 94.92%)\n",
      "\n",
      "  Sentence:   'The new factory is expected to begin operations in the third quarter.'\n",
      "  Sentiment:  Neutral (Confidence: 87.01%)\n",
      "\n",
      "  Sentence:   'Despite a challenging market, the firm maintained its dividend payout.'\n",
      "  Sentiment:  Positive (Confidence: 95.12%)\n",
      "\n",
      "  Sentence:   'Profits have plummeted following the product recall.'\n",
      "  Sentiment:  Negative (Confidence: 97.40%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# First, you need to install the 'transformers' library from Hugging Face.\n",
    "# You can do this by opening your terminal or command prompt and running:\n",
    "# pip install transformers torch\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "def analyze_sentiment(text_list):\n",
    "    \"\"\"\n",
    "    Analyzes the sentiment of a list of financial texts using a pre-trained FinBERT model.\n",
    "\n",
    "    Args:\n",
    "        text_list (list of str): A list of sentences to analyze.\n",
    "\n",
    "    Returns:\n",
    "        list of dict: A list of dictionaries, each containing the original text,\n",
    "                      the predicted sentiment (Positive, Negative, Neutral),\n",
    "                      and the model's confidence score.\n",
    "    \"\"\"\n",
    "    # 1. Load the Pre-trained FinBERT Model and Tokenizer\n",
    "    # =======================================================\n",
    "    # We are using a version of FinBERT that has been specifically fine-tuned for\n",
    "    # sentiment analysis on financial text.\n",
    "    model_name = \"ProsusAI/finbert\"\n",
    "    print(f\"Loading model '{model_name}'... (This may take a moment on first run)\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    # 2. Tokenize the Input Text\n",
    "    # ==========================\n",
    "    # The tokenizer converts our human-readable sentences into a format of numbers (tokens)\n",
    "    # that the model can understand. `padding=True` and `truncation=True` handle\n",
    "    # sentences of different lengths.\n",
    "    inputs = tokenizer(text_list, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # 3. Get Model Predictions\n",
    "    # ========================\n",
    "    # We run the tokenized inputs through the model. `torch.no_grad()` is a performance\n",
    "    # optimization that tells PyTorch we are not training the model, just using it.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # 4. Interpret the Results\n",
    "    # ========================\n",
    "    # The model's output (logits) are raw scores. We apply a softmax function to\n",
    "    # convert them into probabilities between 0 and 1.\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # The model was trained on three labels: positive, negative, neutral.\n",
    "    labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "    \n",
    "    results = []\n",
    "    for i in range(len(text_list)):\n",
    "        # Find the index of the highest probability score\n",
    "        sentiment_idx = torch.argmax(predictions[i]).item()\n",
    "        # Get the corresponding label\n",
    "        sentiment = labels[sentiment_idx]\n",
    "        # Get the confidence score for that prediction\n",
    "        confidence = predictions[i][sentiment_idx].item()\n",
    "        \n",
    "        results.append({\n",
    "            \"text\": text_list[i],\n",
    "            \"sentiment\": sentiment.capitalize(),\n",
    "            \"confidence\": f\"{confidence:.2%}\" # Format as percentage\n",
    "        })\n",
    "        \n",
    "    return results\n",
    "\n",
    "# --- How to use this script ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Define some example financial sentences to analyze\n",
    "    example_sentences = [\n",
    "        \"The company's earnings exceeded expectations, driven by strong sales growth.\",\n",
    "        \"Regulatory concerns and rising costs have negatively impacted our outlook.\",\n",
    "        \"The new factory is expected to begin operations in the third quarter.\",\n",
    "        \"Despite a challenging market, the firm maintained its dividend payout.\",\n",
    "        \"Profits have plummeted following the product recall.\"\n",
    "    ]\n",
    "\n",
    "    # Get the sentiment analysis results\n",
    "    sentiment_results = analyze_sentiment(example_sentences)\n",
    "\n",
    "    # Print the results in a clean format\n",
    "    print(\"\\n--- FinBERT Sentiment Analysis Results ---\")\n",
    "    for result in sentiment_results:\n",
    "        print(f\"  Sentence:   '{result['text']}'\")\n",
    "        print(f\"  Sentiment:  {result['sentiment']} (Confidence: {result['confidence']})\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e37d39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for pickle files in 'C:\\_Files\\Personal\\Projects\\FIAM\\FIAM2025\\data\\text_data'...\n",
      "Found files. Loading the first one: 'text_us_2005.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carte\\AppData\\Local\\Temp\\ipykernel_19912\\3822597473.py:33: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  filings_df = pickle.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a suitable filing for inspection at index 179123.\n",
      "Extracting data for CIK: 879526, Filing Date: 20050303\n",
      "\n",
      "Success! The content has been written to 'filing_example.txt'\n",
      "You can now open this file to see the raw text.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd # Import pandas to handle the DataFrame\n",
    "\n",
    "def inspect_first_filing(data_directory, output_filename=\"filing_example.txt\"):\n",
    "    \"\"\"\n",
    "    Finds the first pickle file, searches for the first row with non-empty text\n",
    "    sections, and saves that filing's text to a file for inspection.\n",
    "\n",
    "    Args:\n",
    "        data_directory (str): The root path where the text data is stored.\n",
    "        output_filename (str): The name of the text file to create.\n",
    "    \"\"\"\n",
    "    print(f\"Searching for pickle files in '{data_directory}'...\")\n",
    "    \n",
    "    # 1. Find the first pickle file automatically\n",
    "    # ============================================\n",
    "    pickle_pattern = os.path.join(data_directory, '**', '*.pkl')\n",
    "    all_pickle_files = glob.glob(pickle_pattern, recursive=True)\n",
    "\n",
    "    if not all_pickle_files:\n",
    "        print(\"Error: No pickle files were found. Please check the directory path.\")\n",
    "        return\n",
    "\n",
    "    first_file_path = all_pickle_files[0]\n",
    "    print(f\"Found files. Loading the first one: '{os.path.basename(first_file_path)}'\")\n",
    "\n",
    "    # 2. Load the data from the pickle file\n",
    "    # =======================================\n",
    "    try:\n",
    "        with open(first_file_path, 'rb') as f:\n",
    "            filings_df = pickle.load(f)\n",
    "            \n",
    "            if filings_df.empty:\n",
    "                print(\"The pickle file contains an empty DataFrame.\")\n",
    "                return\n",
    "\n",
    "            # --- NEW: Loop to find the first valid row ---\n",
    "            # We'll search for the first row that has meaningful content in both key sections.\n",
    "            valid_filing_series = None\n",
    "            for index, row in filings_df.iterrows():\n",
    "                # The correct column names are 'rf' (Risk Factors) and 'mgmt' (MD&A)\n",
    "                risk_text = row.get('rf')\n",
    "                mda_text = row.get('mgmt')\n",
    "\n",
    "                # Check if both sections exist and have a reasonable length (e.g., > 100 characters)\n",
    "                if pd.notna(risk_text) and len(str(risk_text).strip()) > 100 and \\\n",
    "                   pd.notna(mda_text) and len(str(mda_text).strip()) > 100:\n",
    "                    valid_filing_series = row\n",
    "                    print(f\"Found a suitable filing for inspection at index {index}.\")\n",
    "                    break # Exit the loop once a good example is found\n",
    "            \n",
    "            if valid_filing_series is None:\n",
    "                print(\"Error: Could not find a single filing with complete text sections in this file.\")\n",
    "                return\n",
    "\n",
    "            # 3. Extract data from the VALID filing row\n",
    "            # ==========================================================\n",
    "            cik = valid_filing_series.get('cik', 'CIK not found')\n",
    "            filing_date = valid_filing_series.get('date', 'Date not found')\n",
    "            risk_factors_text = valid_filing_series.get('rf', 'Section not found.')\n",
    "            mda_text = valid_filing_series.get('mgmt', 'Section not found.')\n",
    "\n",
    "            print(f\"Extracting data for CIK: {cik}, Filing Date: {filing_date}\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading or processing the pickle file: {e}\")\n",
    "        return\n",
    "\n",
    "    # 4. Write the extracted text to a plain text file\n",
    "    # =================================================\n",
    "    try:\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(f\"EXAMPLE FILING FOR INSPECTION\\n\")\n",
    "            f.write(f\"CIK: {cik} | DATE: {filing_date}\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "            f.write(\"--- RISK FACTORS ---\\n\\n\")\n",
    "            f.write(str(risk_factors_text)) # Use str() to handle potential non-string data\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "            f.write(\"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "            f.write(\"--- MANAGEMENT'S DISCUSSION AND ANALYSIS (MD&A) ---\\n\\n\")\n",
    "            f.write(str(mda_text)) # Use str() to handle potential non-string data\n",
    "        \n",
    "        print(f\"\\nSuccess! The content has been written to '{output_filename}'\")\n",
    "        print(\"You can now open this file to see the raw text.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing the file: {e}\")\n",
    "\n",
    "\n",
    "# --- How to use this script ---\n",
    "if __name__ == \"__main__\":\n",
    "    # !! IMPORTANT: Make sure this path points to your unzipped text data directory !!\n",
    "    text_data_path = r'C:\\_Files\\Personal\\Projects\\FIAM\\FIAM2025\\data\\text_data'\n",
    "    \n",
    "    inspect_first_filing(text_data_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f89f8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for pickle files in 'C:\\_Files\\Personal\\Projects\\FIAM\\FIAM2025\\data\\text_data'...\n",
      "Found files. Loading the first one: 'text_us_2005.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carte\\AppData\\Local\\Temp\\ipykernel_19912\\383267985.py:92: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  filings_df = pickle.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a suitable filing for inspection at index 179123.\n",
      "['Item 7B Risk Factors.................................................................... 24 ITEM 7B - RISK FACTORS', 'You should carefully consider the risks described below in addition to other information contained or incorporated by reference in this Report before investing in our securities. Realization of any of the following risks could have a material adverse effect on our business, financial condition, cash flows and results of operations.', 'RISKS RELATED TO OUR BUSINESS, STRATEGY AND OPERATIONS\\nWE HAVE GENERATED SIGNIFICANT LOSSES IN RECENT PERIODS.', 'We incurred significant net losses during the three years prior to 2004. In 2004, we reported net income of 58.4 million, but we reported net losses of 232.2 million, 56.2 million and 57.2 million for the years ended December 31, 2001, 2002 and 2003, respectively. Our ability to achieve and sustain profitability in the future will depend on the successful continued implementation of measures to reduce costs and achieve sales goals, as well as the ability to pass on to customers increases in raw materials and components. While we have taken steps to lower operating costs and reduce interest expense, and have seen our sales improve in recent periods, we cannot assure you that our cost-reduction measures will be successful, sales will be sustained or increased or that we will achieve a sustained return to profitability.', 'OUR INVENTORIES ARE NOT MANAGED BY PERPETUAL INVENTORY CONTROL SYSTEMS.', 'The systems and processes we use to manage and value our inventories require significant manual intervention and the verification of actual quantities requires physical inventories, which we take several times a year. Breakdowns of these systems and processes, and errors in inventory estimates derived from these systems and processes, could go undetected until the next physical inventory and adversely affect our operations and financial results.', 'AN ADVERSE CHANGE IN OUR CUSTOMER RELATIONSHIPS OR IN THE FINANCIAL CONDITION OF OUR CUSTOMERS COULD ADVERSELY AFFECT OUR BUSINESS.', 'We have relationships with a number of customers where we supply the requirements of these customers. We do not have binding agreements with these customers. Our success is dependent, to a significant extent, upon the continued strength of these relationships and the growth of our core customers. We often are unable to predict the level of demand for our products from these customers, or the timing of their orders. In addition, the same economic conditions that adversely affect us also often adversely affect our customers. As some of our customers are highly leveraged and have limited access to capital, their continued existence may be uncertain. One of our customers, Grupo Transportation Marititma Mexicana SA (TMM), which is located in Mexico, has been experiencing financial difficulties even though it restructured its debt in August 2004, and is in the process of selling certain of its assets. Payments from TMM to us are currently behind schedule. The customer owes us 7.3 million as of December 31, 2004 secured by highly specialized RoadRailer(R) equipment, which due to the nature of the equipment, has a minimal recovery value. The loss of a significant customer or unexpected delays in product purchases could adversely affect our business and results of operations.', 'OUR TECHNOLOGY AND PRODUCTS MAY NOT ACHIEVE MARKET ACCEPTANCE, WHICH COULD ADVERSELY AFFECT OUR COMPETITIVE POSITION.', 'We continue to introduce new products, such as the DuraPlateHD(R) and the Freight-Pro(R) trailer. We cannot assure you that these or other new products or technologies will achieve sustained market acceptance. In addition, new technologies or products that our competitors introduce may render our products obsolete or uncompetitive. We have taken steps to protect our proprietary rights in our new products. However, the steps we have taken to protect them may not be sufficient or may not be enforced by a court of law. If we are unable to protect our proprietary rights, other parties may attempt to copy or otherwise obtain or use our products or technology. If competitors are able to use our technology, our ability to compete effectively could be harmed.', 'WE HAVE A LIMITED NUMBER OF SUPPLIERS OF RAW MATERIALS; AN INCREASE IN THE PRICE OF RAW MATERIALS OR THE INABILITY TO OBTAIN RAW MATERIALS COULD ADVERSELY AFFECT OUR RESULTS OF OPERATIONS.', 'We currently rely on a limited number of suppliers for certain key components in the manufacturing of truck trailers, such as tires, landing gear, axles and specialty steel coil used in DuraPlate(R) panels. From time to time, there have been and may in the future continue to be shortages of supplies of raw materials or our suppliers may place us on allocation, which would have an adverse impact on our ability to meet demand for our products. Raw material shortages and allocations may result in inefficient operations and a build-up of inventory, which can negatively affect our working capital position. In addition, if the price of raw materials were to increase and we were unable to increase our selling prices or reduce our operating costs to offset the price increases, our operating margins would be adversely affected. The loss of any of our suppliers or their inability to meet our price, quality, quantity and delivery requirements could have a significant impact on our results of operations.', 'DISRUPTION OF OUR MANUFACTURING OPERATIONS OR MANAGEMENT INFORMATION SYSTEMS WOULD HAVE AN ADVERSE EFFECT ON OUR FINANCIAL CONDITION AND RESULTS OF OPERATIONS.', 'We manufacture our products at two trailer manufacturing facilities in Lafayette, Indiana, and one hardwood floor facility in Harrison, Arkansas. Our primary manufacturing facility accounts for approximately 85 of our manufacturing output. An unexpected disruption in our production at either of these facilities or in our management information systems for any length of time would have an adverse effect on our business, financial condition and results of operations.', 'THE LOSS OF KEY PERSONNEL COULD ADVERSELY AFFECT OUR RESULTS OF OPERATIONS.', 'Many of our executive officers, including our CEO William P. Greubel and COO Richard J. Giromini, are critical to the management and direction of our business. Our future success depends, in large part, on our ability to retain these officers and other capable management personnel. The unexpected loss of the services of any of our key personnel could have an adverse effect on the operation of our business, as we may be unable to find suitable management to replace departing executives on a timely basis.', 'THE INABILITY TO REALIZE ADDITIONAL COSTS SAVINGS COULD WEAKEN OUR COMPETITIVE POSITION.', 'If we are unable to continue to successfully implement our program of cost reduction and continuous improvement, we may not realize additional anticipated cost savings, which could weaken our competitive position.', 'WE ARE SUBJECT TO CURRENCY EXCHANGE RATE FLUCTUATIONS, WHICH COULD ADVERSELY AFFECT OUR FINANCIAL PERFORMANCE.', 'We are subject to currency exchange rate risk related to sales through our factory-owned retail distribution centers in Canada. For the years ended December 31, 2004 and 2003, currency exchange rate fluctuations had a favorable impact of 0.5 million and 5.3 million, respectively, on our results of operations. We cannot assure you that future currency exchange rate fluctuations will not have an adverse affect on our results of operations.', 'RESTRICTIVE COVENANTS IN OUR DEBT INSTRUMENTS COULD LIMIT OUR FINANCIAL AND OPERATING FLEXIBILITY AND SUBJECT US TO OTHER RISKS.', 'The agreements governing our indebtedness include certain covenants that restrict, among other things, our ability to:', '- incur additional debt;', '- pay dividends on our common stock in excess of 10 million per year;', '- repurchase our common stock;', '- consolidate, merge or transfer all or substantially all of our assets;', '- make certain investments, mergers and acquisitions; and', '- create certain liens.', 'Additionally, should our available borrowing capacity drop below 40 million, we would be subject to a minimum fixed charge coverage ratio which could limit our ability to make capital expenditures and further limit the amount of dividends we could pay.', 'Our ability to comply with such agreements may be affected by events beyond our control, including prevailing economic, financial and industry conditions. In addition, upon the occurrence of an event of default under our debt agreements, the lenders could elect to declare all amounts outstanding under our debt agreements, together with accrued interest, to be immediately due and payable.', 'RISKS PARTICULAR TO THE INDUSTRY IN WHICH WE OPERATE\\nOUR BUSINESS IS HIGHLY CYCLICAL, WHICH COULD ADVERSELY AFFECT OUR SALES AND RESULTS OF OPERATIONS.', \"The truck trailer manufacturing industry historically has been and is expected to continue to be cyclical, as well as affected by overall economic conditions. New trailer production for the trailer industry reached its most recent peak of approximately 306,000 units in 1999, falling to approximately 140,000 by 2001 and rebounding to approximately 229,000 units in 2004. Customers historically have replaced trailers in cycles that run from five to twelve years, depending on service and trailer type. Poor economic conditions can adversely affect demand for new trailers and in the past have led to an overall aging of trailer fleets beyond this typical replacement cycle. Customers' buying patterns can also reflect regulatory changes, such as the new federal hours-of-service rules and anticipated 2007 federal emissions standards. Our business is likely to continue to be highly cyclical based on current and expected economic conditions and regulatory factors.\", 'SIGNIFICANT COMPETITION IN THE INDUSTRY IN WHICH WE OPERATE MAY RESULT IN OUR COMPETITORS OFFERING NEW OR BETTER PRODUCTS AND SERVICES OR LOWER PRICES, WHICH COULD RESULT IN A LOSS OF CUSTOMERS AND A DECREASE IN OUR REVENUES.', 'The truck trailer manufacturing industry is highly competitive. We compete with other manufacturers of varying sizes, some of which may have greater financial resources than we do. Barriers to entry in the standard truck trailer manufacturing industry are low. As a result, it is possible that additional competitors could enter the market at any time. In the recent past, the manufacturing over-capacity and high leverage of some of our competitors, along with the bankruptcies and financial stresses that affected the industry, contributed to significant pricing pressures.', 'If we are unable to compete successfully with other trailer manufacturers, we could lose customers and our revenues may decline. In addition, competitive pressures in the industry may affect the market prices of our new and used equipment, which, in turn, may adversely affect our sales margins and results of operations.', 'WE ARE SUBJECT TO EXTENSIVE GOVERNMENTAL LAWS AND REGULATIONS, AND OUR COSTS RELATED TO COMPLIANCE WITH, OR OUR FAILURE TO COMPLY WITH, EXISTING OR FUTURE LAWS AND REGULATIONS COULD ADVERSELY AFFECT OUR BUSINESS AND RESULTS OF OPERATIONS.', 'The length, height, width, maximum weight capacity and other specifications of truck trailers are regulated by individual states. The Federal government also regulates certain truck trailer safety features, such as lamps, reflective devices, tires, air-brake systems and rear-impact guards. Changes or anticipation of changes in these regulations can have a material impact on our financial results, as our customers may defer purchasing decisions and we may have to reengineer products. In addition, we are subject to various environmental laws and regulations dealing with the transportation, storage, presence, use, disposal and handling of hazardous materials, discharge of storm water and underground fuel storage tanks and may be subject to liability associated with operations of prior owners of acquired property. In 2004, we paid 0.4 million and agreed to a compliance agreement with the EPA related to violations of the federal Clean Water Act at our former Huntsville, Tennessee manufacturing facility.', 'If we are found to be in violation of applicable laws or regulations in the future, it could have an adverse effect on our business, financial condition and results of operations. Our costs of complying with these or any other current or future environmental regulations may be significant. In addition, if we fail to comply with existing or future laws and regulations, we may be subject to governmental or judicial fines or sanctions.', 'A DECLINE IN THE VALUE OF USED TRAILERS COULD ADVERSELY AFFECT OUR RESULTS OF OPERATIONS.', 'General economic and industry conditions, as well as the supply of used trailers, influence the value of used trailers. As part of our normal business practices, we maintain used trailer inventories and have entered into finance contracts secured by used trailers, as well as residual guarantees and purchase commitments for used trailers. Declines in the market value for used trailers or the need to dispose of excess inventories has had, and could in the future have, an adverse effect on our business, financial condition and results of operations.', 'PRODUCT LIABILITY AND OTHER CLAIMS.', 'As a manufacturer of products widely used in commerce, we are subject to regular product liability claims as well as warranty and similar claims alleging defective products. From time to time claims may involve material amounts and novel legal theories, and any insurance we carry may prove inadequate to insulate us from material liabilities for these claims.', 'RISKS RELATED TO AN INVESTMENT IN OUR COMMON STOCK\\nOUR COMMON STOCK HAS EXPERIENCED, AND MAY CONTINUE TO EXPERIENCE, PRICE VOLATILITY AND A LOW TRADING VOLUME.', 'The trading price of our common stock has been and may continue to be subject to large fluctuations. Our common stock price may increase or decrease in response to a number of events and factors, including:', '- trends in our industry and the markets in which we operate;', '- changes in the market price of the products we sell;', '- the introduction of new technologies or products by us or our competitors;', '- changes in expectations as to our future financial performance, including financial estimates by securities analysts and investors;', '- operating results that vary from the expectations of securities analysts and investors;', '- announcements by us or our competitors of significant contracts, acquisitions, strategic partnerships, joint ventures, financings or capital commitments;', '- changes in laws and regulations; and', '- general economic and competitive conditions.', 'This volatility may adversely affect the prices of our common stock regardless of our operating performance. The price of our common stock also may be adversely affected by the amount of common stock issuable upon conversion of our 3 1/4 convertible senior notes due 2008. Assuming 125 million in aggregate principal amount of these notes are converted at a conversion price of 19.20, the number of shares of our common stock outstanding would increase by 6.5 million, or approximately 21 . The conversion feature of our 3 1/4 convertible senior notes is subject to adjustment in connection with the payment of cash dividends. As a result of any future payment of a cash dividend, upon any conversion of the notes we would be required to issue additional shares of common stock.', 'In addition, our common stock has experienced low trading volume in the past.']\n",
      "Extracting data for CIK: 879526, Filing Date: 20050303\n",
      "\n",
      "Success! The content has been written to 'filing_example_titles.txt'\n",
      "You can now open this file to see the raw text with parsed risk factors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re # Import the regular expression module\n",
    "\n",
    "def is_title(line):\n",
    "    \"\"\"\n",
    "    Heuristic to determine if a line of text is a title.\n",
    "    A line is considered a title if it's all uppercase, doesn't end with a period,\n",
    "    and has a reasonable length.\n",
    "    \"\"\"\n",
    "    # Increased the word count limit to 35 to catch longer titles.\n",
    "    return line.isupper() and len(line.split()) < 35 and not line.strip().endswith('.') and len(line) > 5\n",
    "\n",
    "def parse_risk_factors(raw_text):\n",
    "    \"\"\"\n",
    "    Parses the raw text of a 'Risk Factors' section to intelligently combine titles\n",
    "    with their corresponding paragraphs. This version is more robust.\n",
    "\n",
    "    Args:\n",
    "        raw_text (str): The full text of the risk factors section.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string with titles reliably merged with their paragraphs.\n",
    "    \"\"\"\n",
    "    if not isinstance(raw_text, str):\n",
    "        return \"Invalid text input.\"\n",
    "\n",
    "    # Split the entire text block into paragraphs based on one or more empty lines.\n",
    "    raw_paragraphs = re.split(r'\\n\\s*\\n', raw_text.strip())\n",
    "    \n",
    "    # Clean up whitespace, join any internally wrapped lines, and filter out page numbers.\n",
    "    clean_paragraphs = []\n",
    "    for para in raw_paragraphs:\n",
    "        clean_para = ' '.join(para.split())\n",
    "        # Add a check to filter out paragraphs that are just numbers (page numbers)\n",
    "        if clean_para and not clean_para.isdigit():\n",
    "            clean_paragraphs.append(clean_para)\n",
    "\n",
    "    # --- New, More Robust Merging Logic ---\n",
    "    parsed_chunks = []\n",
    "    held_title = \"\"\n",
    "    for para in clean_paragraphs:\n",
    "        if is_title(para):\n",
    "            # If we were already holding a title, it means we found two titles in a row.\n",
    "            # The previous one must have been a standalone title or sub-header.\n",
    "            if held_title:\n",
    "                parsed_chunks.append(held_title)\n",
    "            # Now, hold the new title, waiting for its paragraph.\n",
    "            held_title = para\n",
    "        else: # This is a regular content paragraph.\n",
    "            # If we have a title waiting, prepend it to this paragraph and reset.\n",
    "            if held_title:\n",
    "                parsed_chunks.append(f\"{held_title}\\n{para}\")\n",
    "                held_title = \"\" # Clear the held title as it has been used\n",
    "            else:\n",
    "                # This is just a regular paragraph with no title immediately before it.\n",
    "                parsed_chunks.append(para)\n",
    "\n",
    "    # After the loop, if there's a title left over (i.e., the very last item was a title),\n",
    "    # make sure to add it to the final list.\n",
    "    if held_title:\n",
    "        parsed_chunks.append(held_title)\n",
    "    print(parsed_chunks)  \n",
    "    return \"\\n\\n\".join(parsed_chunks)\n",
    "\n",
    "\n",
    "def inspect_first_filing(data_directory, output_filename=\"filing_example_titles.txt\"):\n",
    "    \"\"\"\n",
    "    Finds the first pickle file, searches for the first row with non-empty text\n",
    "    sections, and saves that filing's text to a file for inspection.\n",
    "    \"\"\"\n",
    "    print(f\"Searching for pickle files in '{data_directory}'...\")\n",
    "    \n",
    "    # 1. Find the first pickle file automatically\n",
    "    # ============================================\n",
    "    pickle_pattern = os.path.join(data_directory, '**', '*.pkl')\n",
    "    all_pickle_files = glob.glob(pickle_pattern, recursive=True)\n",
    "\n",
    "    if not all_pickle_files:\n",
    "        print(\"Error: No pickle files were found. Please check the directory path.\")\n",
    "        return\n",
    "\n",
    "    first_file_path = all_pickle_files[0]\n",
    "    print(f\"Found files. Loading the first one: '{os.path.basename(first_file_path)}'\")\n",
    "\n",
    "    # 2. Load the data from the pickle file\n",
    "    # =======================================\n",
    "    try:\n",
    "        with open(first_file_path, 'rb') as f:\n",
    "            filings_df = pickle.load(f)\n",
    "            \n",
    "            if filings_df.empty:\n",
    "                print(\"The pickle file contains an empty DataFrame.\")\n",
    "                return\n",
    "\n",
    "            valid_filing_series = None\n",
    "            for index, row in filings_df.iterrows():\n",
    "                risk_text = row.get('rf')\n",
    "                mda_text = row.get('mgmt')\n",
    "\n",
    "                if pd.notna(risk_text) and len(str(risk_text).strip()) > 100 and \\\n",
    "                   pd.notna(mda_text) and len(str(mda_text).strip()) > 100:\n",
    "                    valid_filing_series = row\n",
    "                    print(f\"Found a suitable filing for inspection at index {index}.\")\n",
    "                    break\n",
    "            \n",
    "            if valid_filing_series is None:\n",
    "                print(\"Error: Could not find a single filing with complete text sections in this file.\")\n",
    "                return\n",
    "\n",
    "            # 3. Extract data from the VALID filing row\n",
    "            # ==========================================================\n",
    "            cik = valid_filing_series.get('cik', 'CIK not found')\n",
    "            filing_date = valid_filing_series.get('date', 'Date not found')\n",
    "            risk_factors_text_raw = valid_filing_series.get('rf', 'Section not found.')\n",
    "            mda_text = valid_filing_series.get('mgmt', 'Section not found.')\n",
    "            \n",
    "            # --- Use the NEW, more robust parsing function ---\n",
    "            risk_factors_text_parsed = parse_risk_factors(risk_factors_text_raw)\n",
    "\n",
    "            print(f\"Extracting data for CIK: {cik}, Filing Date: {filing_date}\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading or processing the pickle file: {e}\")\n",
    "        return\n",
    "\n",
    "    # 4. Write the extracted text to a plain text file\n",
    "    # =================================================\n",
    "    try:\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(f\"EXAMPLE FILING FOR INSPECTION\\n\")\n",
    "            f.write(f\"CIK: {cik} | DATE: {filing_date}\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "            f.write(\"--- RISK FACTORS (PARSED) ---\\n\\n\")\n",
    "            f.write(str(risk_factors_text_parsed)) # Write the newly parsed text\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "            f.write(\"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "            f.write(\"--- MANAGEMENT'S DISCUSSION AND ANALYSIS (MD&A) ---\\n\\n\")\n",
    "            f.write(str(mda_text))\n",
    "        \n",
    "        print(f\"\\nSuccess! The content has been written to '{output_filename}'\")\n",
    "        print(\"You can now open this file to see the raw text with parsed risk factors.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing the file: {e}\")\n",
    "\n",
    "\n",
    "# --- How to use this script ---\n",
    "if __name__ == \"__main__\":\n",
    "    # !! IMPORTANT: Make sure this path points to your unzipped text data directory !!\n",
    "    text_data_path = r'C:\\_Files\\Personal\\Projects\\FIAM\\FIAM2025\\data\\text_data'\n",
    "    \n",
    "    inspect_first_filing(text_data_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0569317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for pickle files in 'C:\\_Files\\Personal\\Projects\\FIAM\\FIAM2025\\data\\text_data'...\n",
      "Loading file: 'text_us_2005.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carte\\AppData\\Local\\Temp\\ipykernel_19912\\708864471.py:30: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  filings_df = pickle.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > Found valid filing #1 at index 179123.\n",
      "  > Found valid filing #2 at index 179477.\n",
      "  > Found valid filing #3 at index 179927.\n",
      "  > Found valid filing #4 at index 180158.\n",
      "  > Found valid filing #5 at index 180050.\n",
      "  > Found valid filing #6 at index 180115.\n",
      "  > Found valid filing #7 at index 180558.\n",
      "  > Found valid filing #8 at index 180635.\n",
      "  > Found valid filing #9 at index 180394.\n",
      "  > Found valid filing #10 at index 181003.\n",
      "\n",
      "Found 10 valid filings. Writing to 'filing_examples_raw.txt'...\n",
      "Success! The file has been created with raw, unformatted text.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def inspect_multiple_filings(data_directory, num_to_inspect=10, output_filename=\"filing_examples_raw.txt\"):\n",
    "    \"\"\"\n",
    "    Finds and processes a specified number of valid filings and saves their raw,\n",
    "    unformatted risk factor sections to a single text file for comparison.\n",
    "    \"\"\"\n",
    "    print(f\"Searching for pickle files in '{data_directory}'...\")\n",
    "    \n",
    "    pickle_pattern = os.path.join(data_directory, '**', '*.pkl')\n",
    "    all_pickle_files = glob.glob(pickle_pattern, recursive=True)\n",
    "\n",
    "    if not all_pickle_files:\n",
    "        print(\"Error: No pickle files were found. Please check the directory path.\")\n",
    "        return\n",
    "\n",
    "    valid_filings_found = []\n",
    "    \n",
    "    # Loop through the pickle files until we find enough examples\n",
    "    for file_path in all_pickle_files:\n",
    "        if len(valid_filings_found) >= num_to_inspect:\n",
    "            break\n",
    "\n",
    "        print(f\"Loading file: '{os.path.basename(file_path)}'\")\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                filings_df = pickle.load(f)\n",
    "            \n",
    "            if filings_df.empty:\n",
    "                continue # Skip empty dataframes\n",
    "\n",
    "            # Loop through rows to find filings with complete text\n",
    "            for index, row in filings_df.iterrows():\n",
    "                risk_text = row.get('rf')\n",
    "                mda_text = row.get('mgmt')\n",
    "\n",
    "                if pd.notna(risk_text) and len(str(risk_text).strip()) > 100 and \\\n",
    "                   pd.notna(mda_text) and len(str(mda_text).strip()) > 100:\n",
    "                    \n",
    "                    valid_filings_found.append(row)\n",
    "                    print(f\"  > Found valid filing #{len(valid_filings_found)} at index {index}.\")\n",
    "                    \n",
    "                    if len(valid_filings_found) >= num_to_inspect:\n",
    "                        break # Stop searching rows if we have enough\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {os.path.basename(file_path)}: {e}\")\n",
    "\n",
    "    if not valid_filings_found:\n",
    "        print(\"Could not find any valid filings to inspect.\")\n",
    "        return\n",
    "\n",
    "    # Now, write all found examples to a single file\n",
    "    print(f\"\\nFound {len(valid_filings_found)} valid filings. Writing to '{output_filename}'...\")\n",
    "    try:\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            for i, filing_series in enumerate(valid_filings_found):\n",
    "                cik = filing_series.get('cik', 'N/A')\n",
    "                filing_date = filing_series.get('date', 'N/A')\n",
    "                # --- MODIFICATION: Use the raw text directly ---\n",
    "                risk_factors_raw = filing_series.get('rf', '')\n",
    "                \n",
    "                f.write(\"=\"*80 + \"\\n\")\n",
    "                f.write(f\"--- EXAMPLE {i+1} of {len(valid_filings_found)} ---\\n\")\n",
    "                f.write(f\"CIK: {cik} | DATE: {filing_date}\\n\")\n",
    "                f.write(\"=\"*80 + \"\\n\\n\")\n",
    "                \n",
    "                # Write the raw, unparsed text to the file\n",
    "                f.write(str(risk_factors_raw))\n",
    "                f.write(\"\\n\\n\\n\")\n",
    "\n",
    "        print(\"Success! The file has been created with raw, unformatted text.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing the file: {e}\")\n",
    "\n",
    "\n",
    "# --- How to use this script ---\n",
    "if __name__ == \"__main__\":\n",
    "    # !! IMPORTANT: Make sure this path points to your unzipped text data directory !!\n",
    "    text_data_path = r'C:\\_Files\\Personal\\Projects\\FIAM\\FIAM2025\\data\\text_data'\n",
    "    \n",
    "    inspect_multiple_filings(text_data_path, num_to_inspect=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89b0d51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for pickle files in 'C:\\_Files\\Personal\\Projects\\FIAM\\FIAM2025\\data\\text_data'...\n",
      "Loading file: 'text_us_2005.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carte\\AppData\\Local\\Temp\\ipykernel_19912\\1484652006.py:100: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  with open(file_path, 'rb') as f: filings_df = pickle.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > Found valid filing #1 at index 179123.\n",
      "  > Found valid filing #2 at index 179477.\n",
      "  > Found valid filing #3 at index 179927.\n",
      "  > Found valid filing #4 at index 180158.\n",
      "  > Found valid filing #5 at index 180050.\n",
      "  > Found valid filing #6 at index 180115.\n",
      "  > Found valid filing #7 at index 180558.\n",
      "  > Found valid filing #8 at index 180635.\n",
      "  > Found valid filing #9 at index 180394.\n",
      "  > Found valid filing #10 at index 181003.\n",
      "\n",
      "Found 10 valid filings. Writing to 'filing_examples_parsed.txt'...\n",
      "Success! The file has been created with clear separators for each risk chunk.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re # Import the regular expression module\n",
    "\n",
    "def is_header_heuristic(chunk):\n",
    "    \"\"\"\n",
    "    A new, more precise heuristic based on user findings. A chunk is a header if:\n",
    "    1. It is a single sentence.\n",
    "    2. It starts with a capital letter and ends with a period.\n",
    "    3. It is relatively short (under 25 words).\n",
    "    This avoids flagging longer, single-sentence content paragraphs as headers.\n",
    "    \"\"\"\n",
    "    if not isinstance(chunk, str):\n",
    "        return False\n",
    "        \n",
    "    stripped_chunk = chunk.strip()\n",
    "\n",
    "    # Rule 1 & 2: Must be a single sentence starting with a cap and ending with a period.\n",
    "    # We check if there's exactly one period and it's at the end.\n",
    "    is_single_sentence = stripped_chunk.count('.') == 1 and stripped_chunk.endswith('.')\n",
    "    if not is_single_sentence:\n",
    "        return False\n",
    "\n",
    "    # Check that it starts with a capital letter.\n",
    "    if not stripped_chunk[0].isupper():\n",
    "        return False\n",
    "        \n",
    "    # Rule 3: Must be relatively short to be considered a title.\n",
    "    is_short = len(stripped_chunk.split()) < 40\n",
    "    if not is_short:\n",
    "        return False\n",
    "\n",
    "    # If it passes all checks, it's very likely a title.\n",
    "    return True\n",
    "\n",
    "def clean_and_structure_risks(raw_text):\n",
    "    \"\"\"\n",
    "    Intelligently parses raw risk factor text and returns a LIST of structured\n",
    "    risk items, where each item is a header plus its content.\n",
    "    \"\"\"\n",
    "    if not isinstance(raw_text, str):\n",
    "        return []\n",
    "\n",
    "    # 1. Split text into logical chunks based on empty lines.\n",
    "    chunks = re.split(r'\\n\\s*\\n', raw_text.strip())\n",
    "    \n",
    "    # 2. Clean up each chunk and filter out noise.\n",
    "    cleaned_chunks = []\n",
    "    for chunk in chunks:\n",
    "        processed_chunk = ' '.join(chunk.split())\n",
    "        if processed_chunk and not processed_chunk.isdigit():\n",
    "            cleaned_chunks.append(processed_chunk)\n",
    "\n",
    "    # 3. Group chunks into structured risk items (header + content).\n",
    "    if not cleaned_chunks:\n",
    "        return []\n",
    "\n",
    "    structured_items = []\n",
    "    current_item_chunks = []\n",
    "\n",
    "    for chunk in cleaned_chunks:\n",
    "        if is_header_heuristic(chunk):\n",
    "            if current_item_chunks:\n",
    "                structured_items.append(\"\\n\".join(current_item_chunks))\n",
    "            current_item_chunks = [chunk]\n",
    "        else:\n",
    "            if not current_item_chunks:\n",
    "                current_item_chunks = [chunk]\n",
    "            else:\n",
    "                current_item_chunks.append(chunk)\n",
    "    \n",
    "    if current_item_chunks:\n",
    "        structured_items.append(\"\\n\".join(current_item_chunks))\n",
    "\n",
    "    return structured_items\n",
    "\n",
    "\n",
    "def inspect_multiple_filings(data_directory, num_to_inspect=10, output_filename=\"filing_examples_parsed.txt\"):\n",
    "    \"\"\"\n",
    "    Finds and processes a specified number of valid filings and saves their\n",
    "    intelligently parsed risk factor sections to a single text file.\n",
    "    \"\"\"\n",
    "    print(f\"Searching for pickle files in '{data_directory}'...\")\n",
    "    \n",
    "    pickle_pattern = os.path.join(data_directory, '**', '*.pkl')\n",
    "    all_pickle_files = glob.glob(pickle_pattern, recursive=True)\n",
    "\n",
    "    if not all_pickle_files:\n",
    "        print(\"Error: No pickle files were found. Please check the directory path.\")\n",
    "        return\n",
    "\n",
    "    valid_filings_found = []\n",
    "    \n",
    "    for file_path in all_pickle_files:\n",
    "        if len(valid_filings_found) >= num_to_inspect: break\n",
    "        print(f\"Loading file: '{os.path.basename(file_path)}'\")\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f: filings_df = pickle.load(f)\n",
    "            if filings_df.empty: continue\n",
    "\n",
    "            for index, row in filings_df.iterrows():\n",
    "                risk_text = row.get('rf')\n",
    "                mda_text = row.get('mgmt')\n",
    "\n",
    "                if pd.notna(risk_text) and len(str(risk_text).strip()) > 100 and \\\n",
    "                   pd.notna(mda_text) and len(str(mda_text).strip()) > 100:\n",
    "                    valid_filings_found.append(row)\n",
    "                    print(f\"  > Found valid filing #{len(valid_filings_found)} at index {index}.\")\n",
    "                    if len(valid_filings_found) >= num_to_inspect: break\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {os.path.basename(file_path)}: {e}\")\n",
    "\n",
    "    if not valid_filings_found:\n",
    "        print(\"Could not find any valid filings to inspect.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nFound {len(valid_filings_found)} valid filings. Writing to '{output_filename}'...\")\n",
    "    try:\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            for i, filing_series in enumerate(valid_filings_found):\n",
    "                cik = filing_series.get('cik', 'N/A')\n",
    "                filing_date = filing_series.get('date', 'N/A')\n",
    "                risk_factors_raw = filing_series.get('rf', '')\n",
    "                \n",
    "                # This function now returns a list of strings\n",
    "                risk_items_list = clean_and_structure_risks(risk_factors_raw)\n",
    "                \n",
    "                f.write(\"=\"*80 + \"\\n\")\n",
    "                f.write(f\"--- EXAMPLE FILING {i+1} of {len(valid_filings_found)} ---\\n\")\n",
    "                f.write(f\"CIK: {cik} | DATE: {filing_date}\\n\")\n",
    "                f.write(\"=\"*80 + \"\\n\\n\")\n",
    "                \n",
    "                if not risk_items_list:\n",
    "                    f.write(\"--> No risk chunks could be parsed for this filing.\\n\\n\")\n",
    "                else:\n",
    "                    for j, risk_item in enumerate(risk_items_list):\n",
    "                        f.write(f\"---------- Risk Chunk {j+1} ----------\\n\")\n",
    "                        f.write(risk_item)\n",
    "                        f.write(\"\\n\\n\") # Add extra space after each chunk\n",
    "                \n",
    "                f.write(\"\\n\\n\") # Extra space between filings\n",
    "\n",
    "        print(\"Success! The file has been created with clear separators for each risk chunk.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing the file: {e}\")\n",
    "\n",
    "\n",
    "# --- How to use this script ---\n",
    "if __name__ == \"__main__\":\n",
    "    text_data_path = r'C:\\_Files\\Personal\\Projects\\FIAM\\FIAM2025\\data\\text_data'\n",
    "    inspect_multiple_filings(text_data_path, num_to_inspect=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14123fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
